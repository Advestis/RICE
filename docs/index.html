<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>RICE API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Package <code>RICE</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from .RICE import RuleConditions, Rule, RuleSet, Learning

__all__ = [&#34;RuleConditions&#34;, &#34;Rule&#34;, &#34;RuleSet&#34;, &#34;Learning&#34;]</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="RICE.Covering_tools" href="Covering_tools.html">RICE.Covering_tools</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="RICE.RICE" href="RICE.html">RICE.RICE</a></code></dt>
<dd>
<div class="desc"><p>Created on 22 sept. 2016
@author: VMargot</p></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="RICE.Learning"><code class="flex name class">
<span>class <span class="ident">Learning</span></span>
<span>(</span><span>**parameters)</span>
</code></dt>
<dd>
<div class="desc"><p>&hellip;</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>alpha</code></strong> :&ensp;<code>{float type such as 0 &lt; th &lt; 1/4} default 1/5</code></dt>
<dd>The main parameter</dd>
<dt><strong><code>nb_bucket</code></strong> :&ensp;<code>{int type} default max(3, n^1/d) with n the number</code> of <code>row</code></dt>
<dd>and d the number of features
Choose the number a bucket for the discretization</dd>
<dt><strong><code>l_max</code></strong> :&ensp;<code>{int type} default d</code></dt>
<dd>Choose the maximal length of one rule</dd>
<dt><strong><code>gamma</code></strong> :&ensp;<code>{float type such as 0 &lt;= gamma &lt;= 1} default 1</code></dt>
<dd>Choose the maximal intersection rate begin a rule and
a current selected ruleset</dd>
<dt><strong><code>k</code></strong> :&ensp;<code>{int type} default 500</code></dt>
<dd>The maximal number of candidate to increase length</dd>
<dt><strong><code>nb_jobs</code></strong> :&ensp;<code>{int type} default number</code> of <code>core -2</code></dt>
<dd>Select the number of lU used</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Learning(BaseEstimator):
    &#34;&#34;&#34;
    ...
    &#34;&#34;&#34;
    def __init__(self, **parameters):
        &#34;&#34;&#34;

        Parameters
        ----------
        alpha : {float type such as 0 &lt; th &lt; 1/4} default 1/5
                The main parameter

        nb_bucket : {int type} default max(3, n^1/d) with n the number of row
                    and d the number of features
                    Choose the number a bucket for the discretization

        l_max : {int type} default d
                 Choose the maximal length of one rule

        gamma : {float type such as 0 &lt;= gamma &lt;= 1} default 1
                Choose the maximal intersection rate begin a rule and
                a current selected ruleset

        k : {int type} default 500
            The maximal number of candidate to increase length

        nb_jobs : {int type} default number of core -2
                  Select the number of lU used
        &#34;&#34;&#34;
        self.selected_rs = RuleSet([])
        self.ruleset = RuleSet([])
        self.bins = dict()
        self.critlist = []
        self.low_memory = False
        self.k = 150
        self.method = &#39;best&#39;
        self.alpha = 1. / 2 - 1. / 100
        self.gamma = 0.95
        self.nb_jobs = -2
        self.coverage = True

        for arg, val in parameters.items():
            setattr(self, arg, val)

    def __str__(self):
        learning = &#39;Learning&#39;
        # learning = self.get_param(&#39;cpname&#39;) + &#39;: &#39;
        # learning += self.get_param(&#39;target&#39;)
        return learning

    def fit(self, X, y, features_name=None):
        &#34;&#34;&#34;
        Fit the model according to the given training data.

        Parameters
        ----------
        X : {array-like or sparse matrix, shape = [n, d]}
            The training input samples.

        y : {array-like, shape = [n]}
            The target values (real numbers).

        features_name : {list}, optional
                        Name of each features
        &#34;&#34;&#34;
        # Check type for data
        X = check_array(X, dtype=None, force_all_finite=False)  # type: np.ndarray
        y = check_array(y, dtype=None, ensure_2d=False,
                        force_all_finite=False)  # type: np.ndarray

        # Creation of data-driven parameters
        if hasattr(self, &#39;beta&#39;) is False:
            beta = 1. / pow(X.shape[0], 1. / 4 - self.alpha / 2.)
            self.set_params(beta=beta)

        if hasattr(self, &#39;epsilon&#39;) is False:
            beta = self.get_param(&#39;beta&#39;)
            epsilon = beta * np.std(y)
            self.set_params(epsilon=epsilon)

        if hasattr(self, &#39;covmin&#39;) is False:
            covmin = 1. / pow(X.shape[0], self.alpha)
            self.set_params(covmin=covmin)

        if hasattr(self, &#39;nb_bucket&#39;) is False:
            nb_bucket = max(10, int(np.sqrt(pow(X.shape[0],
                                                1. / X.shape[1]))))

            nb_bucket = min(nb_bucket, X.shape[0])
            self.set_params(nb_bucket=nb_bucket)

        if hasattr(self, &#39;covmax&#39;) is False:
            covmax = 1.0
            self.set_params(covmax=covmax)

        if hasattr(self, &#39;calcmethod&#39;) is False:
            if len(set(y)) &gt; 2:
                # Binary classification case
                calcmethod = &#39;mse&#39;
            else:
                # Regression case
                calcmethod = &#39;mae&#39;
            self.set_params(calcmethod=calcmethod)

        features_index = range(X.shape[1])
        if features_name is None:
            features_name = [&#39;X&#39; + str(i) for i in features_index]

        self.set_params(features_index=features_index)
        self.set_params(features_name=features_name)

        if hasattr(self, &#39;l_max&#39;) is False:
            l_max = len(features_name)
            self.set_params(l_max=l_max)

        # Turn the matrix X in a discret matrix
        X_discretized = self.discretize(X)
        self.set_params(X=X_discretized)

        # Normalization of y
        ymean = np.nanmean(y)
        ystd = np.nanstd(y)
        self.set_params(ymean=ymean)
        self.set_params(ystd=ystd)

        self.set_params(y=y)

        # looking for good rules
        self.find_rules()  # works in columns not in lines

        self.set_params(fitted=True)

    def find_rules(self):
        &#34;&#34;&#34;
        Find all rules for all length &lt;= l
        then selects the best subset by minimization
        of the empirical risk
        &#34;&#34;&#34;
        l_max = self.get_param(&#39;l_max&#39;)
        assert l_max &gt; 0, \
            &#39;l_max must be strictly superior to 0&#39;

        selected_rs = self.get_param(&#39;selected_rs&#39;)

        # --------------
        # DESIGNING PART
        # --------------
        self.calc_length_1()
        ruleset = self.get_param(&#39;ruleset&#39;)

        if len(ruleset) &gt; 0:
            for k in range(2, l_max + 1):
                print(&#39;Designing of rules of length %s&#39; % str(k))
                if len(selected_rs.extract_length(k)) == 0:
                    # seeking a set of rules with a length l
                    ruleset_length_up = self.calc_length_c(k)

                    if len(ruleset_length_up) &gt; 0:
                        ruleset += ruleset_length_up
                        self.set_params(ruleset=ruleset)
                    else:
                        print(&#39;No rule for length %s&#39; % str(k))
                        break

                    ruleset.sort_by(&#39;crit&#39;, False)
            self.set_params(ruleset=ruleset)

            # --------------
            # SELECTION PART
            # --------------
            print(&#39;----- Selection ------&#39;)
            selected_rs = self.select_rules(0)

            ruleset.make_rule_names()
            self.set_params(ruleset=ruleset)
            selected_rs.make_rule_names()
            self.set_params(selected_rs=selected_rs)

        else:
            print(&#39;No rule found !&#39;)

    def calc_length_1(self):
        &#34;&#34;&#34;
        Compute all rules of length one and keep the best.
        &#34;&#34;&#34;
        features_name = self.get_param(&#39;features_name&#39;)
        features_index = self.get_param(&#39;features_index&#39;)
        X = self.get_param(&#39;X&#39;)
        calcmethod = self.get_param(&#39;calcmethod&#39;)
        y = self.get_param(&#39;y&#39;)
        cov_max = self.get_param(&#39;covmax&#39;)
        cov_min = self.get_param(&#39;covmin&#39;)
        low_memory = self.get_param(&#39;low_memory&#39;)

        jobs = min(len(features_name), self.get_param(&#39;nb_jobs&#39;))

        if jobs == 1:
            ruleset = list(map(lambda var, idx: make_rules(var, idx, X, y, calcmethod,
                                                           cov_min, cov_max, low_memory),
                               features_name, features_index))
        else:
            ruleset = Parallel(n_jobs=jobs, backend=&#34;multiprocessing&#34;)(
                delayed(make_rules)(var, idx, X, y, calcmethod,
                                    cov_min, cov_max, low_memory)
                for var, idx in zip(features_name, features_index))

        ruleset = functools.reduce(operator.add, ruleset)

        ruleset = RuleSet(ruleset)
        ruleset.sort_by(&#39;crit&#39;, False)

        self.set_params(ruleset=ruleset)

    def calc_length_c(self, length):
        &#34;&#34;&#34;
        Returns a ruleset of rules with a given length.
        &#34;&#34;&#34;
        nb_jobs = self.get_param(&#39;nb_jobs&#39;)
        X = self.get_param(&#39;X&#39;)
        calcmethod = self.get_param(&#39;calcmethod&#39;)
        y = self.get_param(&#39;y&#39;)
        cov_max = self.get_param(&#39;covmax&#39;)
        cov_min = self.get_param(&#39;covmin&#39;)
        low_memory = self.get_param(&#39;low_memory&#39;)

        rules_list = self.find_candidates(length)

        if len(rules_list) &gt; 0:
            if nb_jobs == 1:
                rs = [eval_rule(rule, X, y, calcmethod, cov_min, cov_max, low_memory)
                      for rule in rules_list]
            else:
                rs = Parallel(n_jobs=nb_jobs, backend=&#34;multiprocessing&#34;)(
                    delayed(eval_rule)(rule, X, y, calcmethod,
                                       cov_min, cov_max, low_memory)
                    for rule in rules_list)

            rs = list(filter(None, rs))
            rs_length_up = RuleSet(rs)
            rs_length_up = rs_length_up.drop_duplicates()
            return rs_length_up
        else:
            return []

    def find_candidates(self, length):
        &#34;&#34;&#34;
        Returns the intersection of all suitable rules
        for a given length
        &#34;&#34;&#34;
        rules_list = []
        ruleset = self.get_param(&#39;ruleset&#39;)
        cov_min = self.get_param(&#39;covmin&#39;)
        cov_max = self.get_param(&#39;covmax&#39;)
        k = self.get_param(&#39;k&#39;)
        nb_jobs = self.get_param(&#39;nb_jobs&#39;)
        method = self.get_param(&#39;method&#39;)
        low_memory = self.get_param(&#39;low_memory&#39;)
        if low_memory:
            X = self.get_param(&#39;X&#39;)
        else:
            X = None

        rs1, rs2 = ruleset.get_candidates(X, k, length, method, nb_jobs)
        self.set_params(ruleset=ruleset)

        if len(rs2) &gt; 0:
            inter_list = Parallel(n_jobs=nb_jobs, backend=&#34;multiprocessing&#34;)(
                delayed(calc_intersection)(rule, rs1, cov_min, cov_max,
                                           X, low_memory)
                for rule in rs2)

            inter_list = functools.reduce(operator.add, inter_list)

            inter_list = list(filter(None, inter_list))  # to drop bad rules
            inter_list = list(set(inter_list))  # to drop duplicates

            rules_list += inter_list

        return rules_list

    def select_rules(self, length):
        &#34;&#34;&#34;
        Returns a subset of a given ruleset.
        This subset minimizes the empirical contrast on the learning set
        &#34;&#34;&#34;
        ymean = self.get_param(&#39;ymean&#39;)
        # ystd = self.get_param(&#39;ystd&#39;)
        ruleset = self.get_param(&#39;ruleset&#39;)
        beta = self.get_param(&#39;beta&#39;)
        epsilon = self.get_param(&#39;epsilon&#39;)

        x_train = self.get_param(&#39;X&#39;)

        if length &gt; 0:
            sub_ruleset = ruleset.extract_length(length)
        else:
            sub_ruleset = copy.deepcopy(ruleset)

        print(&#39;Number of rules: %s&#39; % str(len(sub_ruleset)))

        if hasattr(self, &#39;sigma&#39;):
            sigma = self.get_param(&#39;sigma&#39;)
        else:
            sigma = min(sub_ruleset.get_rules_param(&#39;var&#39;))
            self.set_params(sigma=sigma)

        significant_list = list(filter(lambda rule: significant_test(rule, ymean,
                                                                     sigma, beta),
                                       sub_ruleset))
        [rule.set_params(significant=True) for rule in significant_list]
        significant_ruleset = RuleSet(significant_list)
        print(&#39;Number of rules after significant test: %s&#39;
              % str(len(significant_ruleset)))

        if len(significant_ruleset) &gt; 0:
            significant_ruleset.sort_by(&#39;cov&#39;, True)
            # significant_ruleset.sort_by(&#39;crit&#39;, False)
            rg_add, selected_rs = self.select(significant_ruleset)
            print(&#39;Number of selected significant rules: %s&#39; % str(rg_add))

        else:
            selected_rs = None
            print(&#39;No significant rules selected!&#39;)

        if self.coverage:
            # Add insignificant rules
            if selected_rs is None or selected_rs.calc_coverage(x_train) &lt; 1:
                insignificant_list = filter(lambda rule: insignificant_test(rule, sigma,
                                                                            epsilon),
                                            sub_ruleset)
                insignificant_list = list(filter(lambda rule:
                                                 rule not in significant_list,
                                                 insignificant_list))
                if len(list(insignificant_list)) &gt; 0:
                    [rule.set_params(significant=False) for rule in insignificant_list]
                    insignificant_ruleset = RuleSet(insignificant_list)
                    print(&#39;Number rules after insignificant test: %s&#39;
                          % str(len(insignificant_ruleset)))

                    insignificant_ruleset.sort_by(&#39;var&#39;, False)
                    rg_add, selected_rs = self.select(insignificant_ruleset, selected_rs)
                    print(&#39;Number insignificant rules added: %s&#39; % str(rg_add))
                else:
                    print(&#39;No insignificant rule added.&#39;)
            else:
                print(&#39;Covering is completed. No insignificant rule added.&#39;)

            # Add rule to have a covering
            if selected_rs.calc_coverage(x_train) &lt; 1:
                print(&#39;Warning: Covering is not completed!&#39;)
                print(selected_rs.calc_coverage(x_train))
                # neg_rule, pos_rule = add_no_rule(selected_rs, x_train, y_train)
                # features_name = self.get_param(&#39;features_name&#39;)
                #
                # if neg_rule is not None:
                #     id_feature = neg_rule.conditions.get_param(&#39;features_index&#39;)
                #     rule_features = list(itemgetter(*id_feature)(features_name))
                #     neg_rule.conditions.set_params(features_name=rule_features)
                #     neg_rule.calc_stats(y=y_train, x=x_train, cov_min=0.0, cov_max=1.0)
                #     print(&#39;Add negative no-rule  %s.&#39; % str(neg_rule))
                #     selected_rs.append(neg_rule)
                #
                # if pos_rule is not None:
                #     id_feature = pos_rule.conditions.get_param(&#39;features_index&#39;)
                #     rule_features = list(itemgetter(*id_feature)(features_name))
                #     pos_rule.conditions.set_params(features_name=rule_features)
                #     pos_rule.calc_stats(y=y_train, x=x_train, cov_min=0.0, cov_max=1.0)
                #     print(&#39;Add positive no-rule  %s.&#39; % str(pos_rule))
                #     selected_rs.append(pos_rule)
            else:
                print(&#39;Covering is completed.&#39;)

        return selected_rs

    def select(self, rs, selected_rs=None):
        # y_train = self.get_param(&#39;y&#39;)
        # calcmethod = self.get_param(&#39;calcmethod&#39;)
        # crit_evo = self.get_param(&#39;critlist&#39;)
        low_memory = self.get_param(&#39;low_memory&#39;)
        if low_memory:
            x_train = self.get_param(&#39;X&#39;)
        else:
            x_train = None

        if selected_rs is None:
            selected_rs = RuleSet(rs[:1])
            gamma = self.get_param(&#39;gamma&#39;)
            i = 1
            rg_add = 1
        else:
            # gamma = self.get_param(&#39;gamma&#39;)
            gamma = 1.0
            i = 0
            rg_add = 0
        # old_criterion = calc_ruleset_crit(selected_rs, y_train, x_train, calcmethod)
        # crit_evo.append(old_criterion)
        nb_rules = len(rs)

        activation_rs = selected_rs.calc_activation(x_train)
        if low_memory:
            [r.set_params(activation=r.get_activation(x_train)) for r in selected_rs]
        else:
            pass

        while calc_coverage(activation_rs) &lt; 1 and i &lt; nb_rules:
            rs_copy = copy.deepcopy(selected_rs)
            new_rules = rs[i]
            if low_memory:
                new_rules.set_params(activation=new_rules.get_activation(x_train))
            else:
                pass
            union_tests = [new_rules.union_test(rule.get_activation(x_train),
                                                gamma, x_train)
                           for rule in rs_copy]

            if all(union_tests) and \
                    new_rules.union_test(activation_rs, gamma, x_train):
                new_rs = copy.deepcopy(selected_rs)
                new_rs.append(new_rules)
                # new_criterion = calc_ruleset_crit(new_rs, y_train, x_train, calcmethod)

                selected_rs = copy.deepcopy(new_rs)
                activation_rs = selected_rs.calc_activation(x_train)
                # old_criterion = new_criterion
                rg_add += 1

            # crit_evo.append(old_criterion)
            i += 1

        # self.set_params(critlist=crit_evo)
        return rg_add, selected_rs

    def predict(self, X, check_input=True):
        &#34;&#34;&#34;
        Predict regression target for X.
        The predicted regression target of an input sample is computed as the
        application of the selected ruleset on X.

        Parameters
        ----------
        X : {array type or sparse matrix of shape = [n_samples, n_features]}
            The input samples. Internally, its dtype will be converted to
            ``dtype=np.float32``. If a spares matrix is provided, it will be
            converted into a spares ``csr_matrix``.

        check_input : bool type

        Returns
        -------
        y : {array type of shape = [n_samples]}
            The predicted values.
        &#34;&#34;&#34;
        y_train = self.get_param(&#39;y&#39;)
        x_train = self.get_param(&#39;X&#39;)

        X = self.validate_X_predict(X, check_input)
        x_copy = self.discretize(X)

        ruleset = self.get_param(&#39;selected_rs&#39;)

        prediction_vector, bad_cells, no_rules = ruleset.predict(y_train, x_train, x_copy)

        return np.array(prediction_vector), bad_cells, no_rules

    def score(self, x, y, sample_weight=None):
        &#34;&#34;&#34;
        Returns the coefficient of determination R^2 of the prediction
        if y is continuous. Else if y in {0,1} then Returns the mean
        accuracy on the given test data and labels {0,1}.

        Parameters
        ----------
        x : {array type or sparse matrix of shape = [n_samples, n_features]}
            Test samples.

        y : {array type of shape = [n_samples]}
            True values for y.

        sample_weight : {array type of shape = [n_samples], optional
            Sample weights.

        Returns
        -------
        score : float
            R^2 of self.predict(X) wrt. y in R.
            or
        score : float
            Mean accuracy of self.predict(X) wrt. y in {0,1}
        &#34;&#34;&#34;
        x_copy = copy.copy(x)

        prediction_vector = self.predict(x_copy)
        prediction_vector = np.nan_to_num(prediction_vector)

        nan_val = np.argwhere(np.isnan(y))
        if len(nan_val) &gt; 0:
            prediction_vector = np.delete(prediction_vector, nan_val)
            y = np.delete(y, nan_val)

        if len(set(y)) == 2:
            th_val = (min(y) + max(y)) / 2.0
            prediction_vector = list(map(lambda p: min(y) if p &lt; th_val else max(y),
                                         prediction_vector))
            return accuracy_score(y, prediction_vector)
        else:
            return r2_score(y, prediction_vector, sample_weight=sample_weight,
                            multioutput=&#39;variance_weighted&#39;)

    &#34;&#34;&#34;------   Data functions   -----&#34;&#34;&#34;
    def validate_X_predict(self, X, check_input):
        &#34;&#34;&#34;
        Validate X whenever one tries to predict, apply, predict_proba
        &#34;&#34;&#34;
        if hasattr(self, &#39;fitted&#39;) is False:
            raise AttributeError(&#34;Estimator not fitted, &#34;
                                 &#34;call &#39;fit&#39; before exploiting the model.&#34;)

        if check_input:
            X = check_array(X, dtype=None, force_all_finite=False)  # type: np.ndarray

            n_features = X.shape[1]
            input_features = self.get_param(&#39;features_name&#39;)
            if len(input_features) != n_features:
                raise ValueError(&#34;Number of features of the model must &#34;
                                 &#34;match the input. Model n_features is %s and &#34;
                                 &#34;input n_features is %s &#34;
                                 % (input_features, n_features))

        return X

    def discretize(self, x):
        &#34;&#34;&#34;
        Used to have discrete values for each series
        to avoid float

        Parameters
        ----------
        x : {array, matrix type}, shape=[n_samples, n_features]
            Features matrix

        Return
        -------
        col : {array, matrix type}, shape=[n_samples, n_features]
              Features matrix with each features values discretized
              in nb_bucket values
        &#34;&#34;&#34;
        nb_col = x.shape[1]
        nb_bucket = self.get_param(&#39;nb_bucket&#39;)
        bins_dict = self.get_param(&#39;bins&#39;)
        features_name = self.get_param(&#39;features_name&#39;)

        x_mat = []
        for i in range(nb_col):
            xcol = x[:, i]
            try:
                xcol = np.array(xcol.flat, dtype=np.float)
            except ValueError:
                xcol = np.array(xcol.flat, dtype=np.str)

            var_name = features_name[i]

            if np.issubdtype(xcol.dtype, np.floating):
                if var_name not in bins_dict:
                    if len(set(xcol)) &gt;= nb_bucket:
                        bins = find_bins(xcol, nb_bucket)
                        discretized_column = discretize(xcol, nb_bucket, bins)
                        bins_dict[var_name] = bins
                    else:
                        discretized_column = xcol
                else:
                    bins = bins_dict[var_name]
                    discretized_column = discretize(xcol, nb_bucket, bins)
            else:
                discretized_column = xcol

            x_mat.append(discretized_column)

        return np.array(x_mat).T

    def plot_rules(self, var1, var2, length=None,
                   col_pos=&#39;red&#39;, col_neg=&#39;blue&#39;):
        &#34;&#34;&#34;
        Plot the rectangle activation zone of rules in a 2D plot
        the color is corresponding to the intensity of the prediction

        Parameters
        ----------
        var1 : {string type}
               Name of the first variable

        var2 : {string type}
               Name of the second variable

        length : {int type}, optional
                 Option to plot only the length 1 or length 2 rules

        col_pos : {string type}, optional,
                  Name of the color of the zone of positive rules

        col_neg : {string type}, optional
                  Name of the color of the zone of negative rules

        -------
        Draw the graphic
        &#34;&#34;&#34;
        selected_rs = self.get_param(&#39;selected_rs&#39;)
        nb_bucket = self.get_param(&#39;nb_bucket&#39;)

        if length is not None:
            sub_ruleset = selected_rs.extract_length(length)
        else:
            sub_ruleset = selected_rs

        plt.plot()

        for rule in sub_ruleset:
            rule_condition = rule.conditions

            var = rule_condition.get_param(&#39;features_index&#39;)
            bmin = rule_condition.get_param(&#39;bmin&#39;)
            bmax = rule_condition.get_param(&#39;bmax&#39;)
            length_rule = rule.get_param(&#39;length&#39;)

            if rule.get_param(&#39;pred&#39;) &gt; 0:
                hatch = &#39;/&#39;
                facecolor = col_pos
                alpha = min(1, abs(rule.get_param(&#39;pred&#39;)) / 2.0)
            else:
                hatch = &#39;\\&#39;
                facecolor = col_neg
                alpha = min(1, abs(rule.get_param(&#39;pred&#39;)) / 2.0)

            if length_rule == 1:
                if var[0] == var1:
                    p = patches.Rectangle((bmin[0], 0),  # origin
                                          (bmax[0] - bmin[0]) + 0.99,  # width
                                          nb_bucket,  # height
                                          hatch=hatch, facecolor=facecolor,
                                          alpha=alpha)
                    plt.gca().add_patch(p)

                elif var[0] == var2:
                    p = patches.Rectangle((0, bmin[0]),
                                          nb_bucket,
                                          (bmax[0] - bmin[0]) + 0.99,
                                          hatch=hatch, facecolor=facecolor,
                                          alpha=alpha)
                    plt.gca().add_patch(p)

            elif length_rule == 2:
                if var[0] == var1 and var[1] == var2:
                    p = patches.Rectangle((bmin[0], bmin[1]),
                                          (bmax[0] - bmin[0]) + 0.99,
                                          (bmax[1] - bmin[1]) + 0.99,
                                          hatch=hatch, facecolor=facecolor,
                                          alpha=alpha)
                    plt.gca().add_patch(p)

                elif var[1] == var1 and var[0] == var2:
                    p = patches.Rectangle((bmin[1], bmin[0]),
                                          (bmax[1] - bmin[1]) + 0.99,
                                          (bmax[0] - bmin[0]) + 0.99,
                                          hatch=hatch, facecolor=facecolor,
                                          alpha=alpha)
                    plt.gca().add_patch(p)

        if length is None:
            plt.gca().set_title(&#39;rules activations&#39;)
        else:
            plt.gca().set_title(&#39;rules l%s activations&#39; % str(length))

        plt.gca().axis([-0.1, nb_bucket + 0.1, -0.1, nb_bucket + 0.1])

    def plot_pred(self, x, y, var1, var2, cmap=None,
                  vmin=None, vmax=None, add_points=True,
                  add_score=False):
        &#34;&#34;&#34;
        Plot the prediction zone of rules in a 2D plot

        Parameters
        ----------
        x : {array-like, sparse matrix}, shape=[n_samples, n_features]
            Features matrix, where n_samples in the number of samples and
            n_features is the number of features.

        y : {array-like}, shape=[n_samples]
            Target vector relative to X

        var1 : {int type}
               Number of the column of the first variable

        var2 : {int type}
               Number of the column of the second variable

        cmap : {colormap object}, optional
               Colormap used for the graphic

        vmax, vmin : {float type}, optional
                     Parameter of the range of the colorbar

        add_points: {boolean type}, optional
                    Option to add the discrete scatter of y

        add_score : {boolean type}, optional
                    Option to add the score on the graphic

        -------
        Draw the graphic
        &#34;&#34;&#34;
        nb_bucket = self.get_param(&#39;nb_bucket&#39;)
        x_discretized = self.discretize(x)
        selected_rs = self.get_param(&#39;selected_rs&#39;)
        y_train = self.get_param(&#39;y&#39;)
        ymean = self.get_param(&#39;ymean&#39;)
        ystd = self.get_param(&#39;ystd&#39;)

        x1 = x_discretized[:, var1]
        x2 = x_discretized[:, var2]

        xx, yy = np.meshgrid(range(nb_bucket),
                             range(nb_bucket))

        if cmap is None:
            cmap = plt.cm.get_cmap(&#39;coolwarm&#39;)

        z = selected_rs.predict(y_train, np.c_[np.round(xx.ravel()),
                                               np.round(yy.ravel())],
                                ymean, ystd)

        if vmin is None:
            vmin = min(z)
        if vmax is None:
            vmax = max(z)

        z = z.reshape(xx.shape)

        plt.contourf(xx, yy, z, cmap=cmap, alpha=.8, vmax=vmax, vmin=vmin)

        if add_points:
            area = map(lambda b:
                       map(lambda a:
                           np.extract(np.logical_and(x1 == a, x2 == b),
                                      y).mean(), range(nb_bucket)),
                       range(nb_bucket))
            area = list(area)

            area_len = map(lambda b:
                           map(lambda a:
                               len(np.extract(np.logical_and(x1 == a, x2 == b),
                                              y)) * 10, range(nb_bucket)),
                           range(nb_bucket))
            area_len = list(area_len)

            plt.scatter(xx, yy, c=area, s=area_len, alpha=1.0,
                        cmap=cmap, vmax=vmax, vmin=vmin)

        plt.title(&#39;RIPE prediction&#39;)

        if add_score:
            score = self.score(x, y)
            plt.text(nb_bucket - .70, .08, (&#39;%.2f&#39; % str(score)).lstrip(&#39;0&#39;),
                     size=20, horizontalalignment=&#39;right&#39;)

        plt.axis([-0.01, nb_bucket - 0.99, -0.01, nb_bucket - 0.99])
        plt.colorbar()

    def plot_counter_variables(self):
        &#34;&#34;&#34;
        Function plots a graphical counter of variables used in rules.
        &#34;&#34;&#34;
        rs = self.get_param(&#39;selected_rs&#39;)
        f = rs.plot_counter_variables()

        return f

    def plot_counter(self):
        &#34;&#34;&#34;
        Function plots a graphical counter of variables used in rules by modality.
        &#34;&#34;&#34;
        nb_bucket = self.get_param(&#39;nb_bucket&#39;)
        y_labels, counter = self.make_count_matrix(return_vars=True)

        x_labels = list(map(lambda i: str(i), range(nb_bucket)))

        f = plt.figure()
        ax = plt.subplot()

        g = sns.heatmap(counter, xticklabels=x_labels, yticklabels=y_labels,
                        cmap=&#39;Reds&#39;, linewidths=.05, ax=ax, center=0.0)
        g.xaxis.tick_top()
        plt.yticks(rotation=0)

        return f

    def plot_dist(self, x=None):
        &#34;&#34;&#34;
        Function plots a graphical correlation of rules.
        &#34;&#34;&#34;
        rs = self.get_param(&#39;selected_rs&#39;)
        if x is None and self.get_param(&#39;low_memory&#39;):
            x = self.get_param(&#39;X&#39;)

        f = rs.plot_dist(x=x)

        return f

    def plot_intensity(self):
        &#34;&#34;&#34;
        Function plots a graphical counter of variables used in rules.
        &#34;&#34;&#34;
        y_labels, counter = self.make_count_matrix(return_vars=True)
        intensity = self.make_count_matrix(add_pred=True)

        nb_bucket = self.get_param(&#39;nb_bucket&#39;)
        x_labels = [str(i) for i in range(nb_bucket)]

        with np.errstate(divide=&#39;ignore&#39;, invalid=&#39;ignore&#39;):
            val = np.divide(intensity, counter)

        val[np.isneginf(val)] = np.nan
        val = np.nan_to_num(val)

        f = plt.figure()
        ax = plt.subplot()

        g = sns.heatmap(val, xticklabels=x_labels, yticklabels=y_labels,
                        cmap=&#39;bwr&#39;, linewidths=.05, ax=ax, center=0.0)
        g.xaxis.tick_top()
        plt.yticks(rotation=0)

        return f

    def make_count_matrix(self, add_pred=False, return_vars=False):
        &#34;&#34;&#34;
        Return a count matrix of each variable in each modality
        &#34;&#34;&#34;
        ruleset = self.get_param(&#39;selected_rs&#39;)
        nb_bucket = self.get_param(&#39;nb_bucket&#39;)

        counter = get_variables_count(ruleset)

        vars_list = [item[0] for item in counter]

        count_mat = np.zeros((nb_bucket, len(vars_list)))
        str_id = []

        for rule in ruleset:
            cd = rule.conditions
            var_name = cd.get_param(&#39;features_name&#39;)
            bmin = cd.get_param(&#39;bmin&#39;)
            bmax = cd.get_param(&#39;bmax&#39;)

            for j in range(len(var_name)):
                if type(bmin[j]) != str:
                    for b in range(int(bmin[j]), int(bmax[j]) + 1):
                        var_id = vars_list.index(var_name[j])
                        if add_pred:
                            count_mat[b, var_id] += rule.get_param(&#39;pred&#39;)
                        else:
                            count_mat[b, var_id] += 1
                else:
                    str_id += [vars_list.index(var_name[j])]

        vars_list = [i for j, i in enumerate(vars_list) if j not in str_id]
        count_mat = np.delete(count_mat.T, str_id, 0)

        if return_vars:
            return vars_list, count_mat
        else:
            return count_mat

    def make_selected_df(self):
        &#34;&#34;&#34;
        Returns
        -------
        selected_df : {DataFrame type}
                      DataFrame of selected RuleSet for presentation
        &#34;&#34;&#34;
        selected_rs = self.get_param(&#39;selected_rs&#39;)
        selected_df = selected_rs.make_selected_df()
        return selected_df

    &#34;&#34;&#34;------   Getters   -----&#34;&#34;&#34;
    def get_param(self, param):
        &#34;&#34;&#34;
        To get the parameter param
        &#34;&#34;&#34;
        assert type(param) == str, &#39;Must be a string&#39;
        if hasattr(self, param):
            return getattr(self, param)
        else:
            return None

    &#34;&#34;&#34;------   Setters   -----&#34;&#34;&#34;
    def set_params(self, **parameters):
        &#34;&#34;&#34;
        To set a new parameter
        Example:
        --------
        o.set_params(new_param=val_new_param)
        &#34;&#34;&#34;
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        return self</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.base.BaseEstimator</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="RICE.Learning.calc_length_1"><code class="name flex">
<span>def <span class="ident">calc_length_1</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute all rules of length one and keep the best.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calc_length_1(self):
    &#34;&#34;&#34;
    Compute all rules of length one and keep the best.
    &#34;&#34;&#34;
    features_name = self.get_param(&#39;features_name&#39;)
    features_index = self.get_param(&#39;features_index&#39;)
    X = self.get_param(&#39;X&#39;)
    calcmethod = self.get_param(&#39;calcmethod&#39;)
    y = self.get_param(&#39;y&#39;)
    cov_max = self.get_param(&#39;covmax&#39;)
    cov_min = self.get_param(&#39;covmin&#39;)
    low_memory = self.get_param(&#39;low_memory&#39;)

    jobs = min(len(features_name), self.get_param(&#39;nb_jobs&#39;))

    if jobs == 1:
        ruleset = list(map(lambda var, idx: make_rules(var, idx, X, y, calcmethod,
                                                       cov_min, cov_max, low_memory),
                           features_name, features_index))
    else:
        ruleset = Parallel(n_jobs=jobs, backend=&#34;multiprocessing&#34;)(
            delayed(make_rules)(var, idx, X, y, calcmethod,
                                cov_min, cov_max, low_memory)
            for var, idx in zip(features_name, features_index))

    ruleset = functools.reduce(operator.add, ruleset)

    ruleset = RuleSet(ruleset)
    ruleset.sort_by(&#39;crit&#39;, False)

    self.set_params(ruleset=ruleset)</code></pre>
</details>
</dd>
<dt id="RICE.Learning.calc_length_c"><code class="name flex">
<span>def <span class="ident">calc_length_c</span></span>(<span>self, length)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a ruleset of rules with a given length.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calc_length_c(self, length):
    &#34;&#34;&#34;
    Returns a ruleset of rules with a given length.
    &#34;&#34;&#34;
    nb_jobs = self.get_param(&#39;nb_jobs&#39;)
    X = self.get_param(&#39;X&#39;)
    calcmethod = self.get_param(&#39;calcmethod&#39;)
    y = self.get_param(&#39;y&#39;)
    cov_max = self.get_param(&#39;covmax&#39;)
    cov_min = self.get_param(&#39;covmin&#39;)
    low_memory = self.get_param(&#39;low_memory&#39;)

    rules_list = self.find_candidates(length)

    if len(rules_list) &gt; 0:
        if nb_jobs == 1:
            rs = [eval_rule(rule, X, y, calcmethod, cov_min, cov_max, low_memory)
                  for rule in rules_list]
        else:
            rs = Parallel(n_jobs=nb_jobs, backend=&#34;multiprocessing&#34;)(
                delayed(eval_rule)(rule, X, y, calcmethod,
                                   cov_min, cov_max, low_memory)
                for rule in rules_list)

        rs = list(filter(None, rs))
        rs_length_up = RuleSet(rs)
        rs_length_up = rs_length_up.drop_duplicates()
        return rs_length_up
    else:
        return []</code></pre>
</details>
</dd>
<dt id="RICE.Learning.discretize"><code class="name flex">
<span>def <span class="ident">discretize</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Used to have discrete values for each series
to avoid float</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>{array, matrix type}, shape=[n_samples, n_features]</code></dt>
<dd>Features matrix</dd>
</dl>
<h2 id="return">Return</h2>
<p>col : {array, matrix type}, shape=[n_samples, n_features]
Features matrix with each features values discretized
in nb_bucket values</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def discretize(self, x):
    &#34;&#34;&#34;
    Used to have discrete values for each series
    to avoid float

    Parameters
    ----------
    x : {array, matrix type}, shape=[n_samples, n_features]
        Features matrix

    Return
    -------
    col : {array, matrix type}, shape=[n_samples, n_features]
          Features matrix with each features values discretized
          in nb_bucket values
    &#34;&#34;&#34;
    nb_col = x.shape[1]
    nb_bucket = self.get_param(&#39;nb_bucket&#39;)
    bins_dict = self.get_param(&#39;bins&#39;)
    features_name = self.get_param(&#39;features_name&#39;)

    x_mat = []
    for i in range(nb_col):
        xcol = x[:, i]
        try:
            xcol = np.array(xcol.flat, dtype=np.float)
        except ValueError:
            xcol = np.array(xcol.flat, dtype=np.str)

        var_name = features_name[i]

        if np.issubdtype(xcol.dtype, np.floating):
            if var_name not in bins_dict:
                if len(set(xcol)) &gt;= nb_bucket:
                    bins = find_bins(xcol, nb_bucket)
                    discretized_column = discretize(xcol, nb_bucket, bins)
                    bins_dict[var_name] = bins
                else:
                    discretized_column = xcol
            else:
                bins = bins_dict[var_name]
                discretized_column = discretize(xcol, nb_bucket, bins)
        else:
            discretized_column = xcol

        x_mat.append(discretized_column)

    return np.array(x_mat).T</code></pre>
</details>
</dd>
<dt id="RICE.Learning.find_candidates"><code class="name flex">
<span>def <span class="ident">find_candidates</span></span>(<span>self, length)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the intersection of all suitable rules
for a given length</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_candidates(self, length):
    &#34;&#34;&#34;
    Returns the intersection of all suitable rules
    for a given length
    &#34;&#34;&#34;
    rules_list = []
    ruleset = self.get_param(&#39;ruleset&#39;)
    cov_min = self.get_param(&#39;covmin&#39;)
    cov_max = self.get_param(&#39;covmax&#39;)
    k = self.get_param(&#39;k&#39;)
    nb_jobs = self.get_param(&#39;nb_jobs&#39;)
    method = self.get_param(&#39;method&#39;)
    low_memory = self.get_param(&#39;low_memory&#39;)
    if low_memory:
        X = self.get_param(&#39;X&#39;)
    else:
        X = None

    rs1, rs2 = ruleset.get_candidates(X, k, length, method, nb_jobs)
    self.set_params(ruleset=ruleset)

    if len(rs2) &gt; 0:
        inter_list = Parallel(n_jobs=nb_jobs, backend=&#34;multiprocessing&#34;)(
            delayed(calc_intersection)(rule, rs1, cov_min, cov_max,
                                       X, low_memory)
            for rule in rs2)

        inter_list = functools.reduce(operator.add, inter_list)

        inter_list = list(filter(None, inter_list))  # to drop bad rules
        inter_list = list(set(inter_list))  # to drop duplicates

        rules_list += inter_list

    return rules_list</code></pre>
</details>
</dd>
<dt id="RICE.Learning.find_rules"><code class="name flex">
<span>def <span class="ident">find_rules</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Find all rules for all length &lt;= l
then selects the best subset by minimization
of the empirical risk</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_rules(self):
    &#34;&#34;&#34;
    Find all rules for all length &lt;= l
    then selects the best subset by minimization
    of the empirical risk
    &#34;&#34;&#34;
    l_max = self.get_param(&#39;l_max&#39;)
    assert l_max &gt; 0, \
        &#39;l_max must be strictly superior to 0&#39;

    selected_rs = self.get_param(&#39;selected_rs&#39;)

    # --------------
    # DESIGNING PART
    # --------------
    self.calc_length_1()
    ruleset = self.get_param(&#39;ruleset&#39;)

    if len(ruleset) &gt; 0:
        for k in range(2, l_max + 1):
            print(&#39;Designing of rules of length %s&#39; % str(k))
            if len(selected_rs.extract_length(k)) == 0:
                # seeking a set of rules with a length l
                ruleset_length_up = self.calc_length_c(k)

                if len(ruleset_length_up) &gt; 0:
                    ruleset += ruleset_length_up
                    self.set_params(ruleset=ruleset)
                else:
                    print(&#39;No rule for length %s&#39; % str(k))
                    break

                ruleset.sort_by(&#39;crit&#39;, False)
        self.set_params(ruleset=ruleset)

        # --------------
        # SELECTION PART
        # --------------
        print(&#39;----- Selection ------&#39;)
        selected_rs = self.select_rules(0)

        ruleset.make_rule_names()
        self.set_params(ruleset=ruleset)
        selected_rs.make_rule_names()
        self.set_params(selected_rs=selected_rs)

    else:
        print(&#39;No rule found !&#39;)</code></pre>
</details>
</dd>
<dt id="RICE.Learning.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X, y, features_name=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Fit the model according to the given training data.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>{array-like</code> or <code>sparse matrix, shape = [n, d]}</code></dt>
<dd>The training input samples.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>{array-like, shape = [n]}</code></dt>
<dd>The target values (real numbers).</dd>
<dt><strong><code>features_name</code></strong> :&ensp;<code>{list}</code>, optional</dt>
<dd>Name of each features</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X, y, features_name=None):
    &#34;&#34;&#34;
    Fit the model according to the given training data.

    Parameters
    ----------
    X : {array-like or sparse matrix, shape = [n, d]}
        The training input samples.

    y : {array-like, shape = [n]}
        The target values (real numbers).

    features_name : {list}, optional
                    Name of each features
    &#34;&#34;&#34;
    # Check type for data
    X = check_array(X, dtype=None, force_all_finite=False)  # type: np.ndarray
    y = check_array(y, dtype=None, ensure_2d=False,
                    force_all_finite=False)  # type: np.ndarray

    # Creation of data-driven parameters
    if hasattr(self, &#39;beta&#39;) is False:
        beta = 1. / pow(X.shape[0], 1. / 4 - self.alpha / 2.)
        self.set_params(beta=beta)

    if hasattr(self, &#39;epsilon&#39;) is False:
        beta = self.get_param(&#39;beta&#39;)
        epsilon = beta * np.std(y)
        self.set_params(epsilon=epsilon)

    if hasattr(self, &#39;covmin&#39;) is False:
        covmin = 1. / pow(X.shape[0], self.alpha)
        self.set_params(covmin=covmin)

    if hasattr(self, &#39;nb_bucket&#39;) is False:
        nb_bucket = max(10, int(np.sqrt(pow(X.shape[0],
                                            1. / X.shape[1]))))

        nb_bucket = min(nb_bucket, X.shape[0])
        self.set_params(nb_bucket=nb_bucket)

    if hasattr(self, &#39;covmax&#39;) is False:
        covmax = 1.0
        self.set_params(covmax=covmax)

    if hasattr(self, &#39;calcmethod&#39;) is False:
        if len(set(y)) &gt; 2:
            # Binary classification case
            calcmethod = &#39;mse&#39;
        else:
            # Regression case
            calcmethod = &#39;mae&#39;
        self.set_params(calcmethod=calcmethod)

    features_index = range(X.shape[1])
    if features_name is None:
        features_name = [&#39;X&#39; + str(i) for i in features_index]

    self.set_params(features_index=features_index)
    self.set_params(features_name=features_name)

    if hasattr(self, &#39;l_max&#39;) is False:
        l_max = len(features_name)
        self.set_params(l_max=l_max)

    # Turn the matrix X in a discret matrix
    X_discretized = self.discretize(X)
    self.set_params(X=X_discretized)

    # Normalization of y
    ymean = np.nanmean(y)
    ystd = np.nanstd(y)
    self.set_params(ymean=ymean)
    self.set_params(ystd=ystd)

    self.set_params(y=y)

    # looking for good rules
    self.find_rules()  # works in columns not in lines

    self.set_params(fitted=True)</code></pre>
</details>
</dd>
<dt id="RICE.Learning.get_param"><code class="name flex">
<span>def <span class="ident">get_param</span></span>(<span>self, param)</span>
</code></dt>
<dd>
<div class="desc"><p>To get the parameter param</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_param(self, param):
    &#34;&#34;&#34;
    To get the parameter param
    &#34;&#34;&#34;
    assert type(param) == str, &#39;Must be a string&#39;
    if hasattr(self, param):
        return getattr(self, param)
    else:
        return None</code></pre>
</details>
</dd>
<dt id="RICE.Learning.make_count_matrix"><code class="name flex">
<span>def <span class="ident">make_count_matrix</span></span>(<span>self, add_pred=False, return_vars=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Return a count matrix of each variable in each modality</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_count_matrix(self, add_pred=False, return_vars=False):
    &#34;&#34;&#34;
    Return a count matrix of each variable in each modality
    &#34;&#34;&#34;
    ruleset = self.get_param(&#39;selected_rs&#39;)
    nb_bucket = self.get_param(&#39;nb_bucket&#39;)

    counter = get_variables_count(ruleset)

    vars_list = [item[0] for item in counter]

    count_mat = np.zeros((nb_bucket, len(vars_list)))
    str_id = []

    for rule in ruleset:
        cd = rule.conditions
        var_name = cd.get_param(&#39;features_name&#39;)
        bmin = cd.get_param(&#39;bmin&#39;)
        bmax = cd.get_param(&#39;bmax&#39;)

        for j in range(len(var_name)):
            if type(bmin[j]) != str:
                for b in range(int(bmin[j]), int(bmax[j]) + 1):
                    var_id = vars_list.index(var_name[j])
                    if add_pred:
                        count_mat[b, var_id] += rule.get_param(&#39;pred&#39;)
                    else:
                        count_mat[b, var_id] += 1
            else:
                str_id += [vars_list.index(var_name[j])]

    vars_list = [i for j, i in enumerate(vars_list) if j not in str_id]
    count_mat = np.delete(count_mat.T, str_id, 0)

    if return_vars:
        return vars_list, count_mat
    else:
        return count_mat</code></pre>
</details>
</dd>
<dt id="RICE.Learning.make_selected_df"><code class="name flex">
<span>def <span class="ident">make_selected_df</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>selected_df</code></strong> :&ensp;<code>{DataFrame type}</code></dt>
<dd>DataFrame of selected RuleSet for presentation</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_selected_df(self):
    &#34;&#34;&#34;
    Returns
    -------
    selected_df : {DataFrame type}
                  DataFrame of selected RuleSet for presentation
    &#34;&#34;&#34;
    selected_rs = self.get_param(&#39;selected_rs&#39;)
    selected_df = selected_rs.make_selected_df()
    return selected_df</code></pre>
</details>
</dd>
<dt id="RICE.Learning.plot_counter"><code class="name flex">
<span>def <span class="ident">plot_counter</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Function plots a graphical counter of variables used in rules by modality.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_counter(self):
    &#34;&#34;&#34;
    Function plots a graphical counter of variables used in rules by modality.
    &#34;&#34;&#34;
    nb_bucket = self.get_param(&#39;nb_bucket&#39;)
    y_labels, counter = self.make_count_matrix(return_vars=True)

    x_labels = list(map(lambda i: str(i), range(nb_bucket)))

    f = plt.figure()
    ax = plt.subplot()

    g = sns.heatmap(counter, xticklabels=x_labels, yticklabels=y_labels,
                    cmap=&#39;Reds&#39;, linewidths=.05, ax=ax, center=0.0)
    g.xaxis.tick_top()
    plt.yticks(rotation=0)

    return f</code></pre>
</details>
</dd>
<dt id="RICE.Learning.plot_counter_variables"><code class="name flex">
<span>def <span class="ident">plot_counter_variables</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Function plots a graphical counter of variables used in rules.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_counter_variables(self):
    &#34;&#34;&#34;
    Function plots a graphical counter of variables used in rules.
    &#34;&#34;&#34;
    rs = self.get_param(&#39;selected_rs&#39;)
    f = rs.plot_counter_variables()

    return f</code></pre>
</details>
</dd>
<dt id="RICE.Learning.plot_dist"><code class="name flex">
<span>def <span class="ident">plot_dist</span></span>(<span>self, x=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Function plots a graphical correlation of rules.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_dist(self, x=None):
    &#34;&#34;&#34;
    Function plots a graphical correlation of rules.
    &#34;&#34;&#34;
    rs = self.get_param(&#39;selected_rs&#39;)
    if x is None and self.get_param(&#39;low_memory&#39;):
        x = self.get_param(&#39;X&#39;)

    f = rs.plot_dist(x=x)

    return f</code></pre>
</details>
</dd>
<dt id="RICE.Learning.plot_intensity"><code class="name flex">
<span>def <span class="ident">plot_intensity</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Function plots a graphical counter of variables used in rules.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_intensity(self):
    &#34;&#34;&#34;
    Function plots a graphical counter of variables used in rules.
    &#34;&#34;&#34;
    y_labels, counter = self.make_count_matrix(return_vars=True)
    intensity = self.make_count_matrix(add_pred=True)

    nb_bucket = self.get_param(&#39;nb_bucket&#39;)
    x_labels = [str(i) for i in range(nb_bucket)]

    with np.errstate(divide=&#39;ignore&#39;, invalid=&#39;ignore&#39;):
        val = np.divide(intensity, counter)

    val[np.isneginf(val)] = np.nan
    val = np.nan_to_num(val)

    f = plt.figure()
    ax = plt.subplot()

    g = sns.heatmap(val, xticklabels=x_labels, yticklabels=y_labels,
                    cmap=&#39;bwr&#39;, linewidths=.05, ax=ax, center=0.0)
    g.xaxis.tick_top()
    plt.yticks(rotation=0)

    return f</code></pre>
</details>
</dd>
<dt id="RICE.Learning.plot_pred"><code class="name flex">
<span>def <span class="ident">plot_pred</span></span>(<span>self, x, y, var1, var2, cmap=None, vmin=None, vmax=None, add_points=True, add_score=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot the prediction zone of rules in a 2D plot</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>{array-like, sparse matrix}, shape=[n_samples, n_features]</code></dt>
<dd>Features matrix, where n_samples in the number of samples and
n_features is the number of features.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>{array-like}, shape=[n_samples]</code></dt>
<dd>Target vector relative to X</dd>
<dt><strong><code>var1</code></strong> :&ensp;<code>{int type}</code></dt>
<dd>Number of the column of the first variable</dd>
<dt><strong><code>var2</code></strong> :&ensp;<code>{int type}</code></dt>
<dd>Number of the column of the second variable</dd>
<dt><strong><code>cmap</code></strong> :&ensp;<code>{colormap object}</code>, optional</dt>
<dd>Colormap used for the graphic</dd>
<dt><strong><code>vmax</code></strong>, <strong><code>vmin</code></strong> :&ensp;<code>{float type}</code>, optional</dt>
<dd>Parameter of the range of the colorbar</dd>
<dt><strong><code>add_points</code></strong> :&ensp;<code>{boolean type}</code>, optional</dt>
<dd>Option to add the discrete scatter of y</dd>
<dt><strong><code>add_score</code></strong> :&ensp;<code>{boolean type}</code>, optional</dt>
<dd>Option to add the score on the graphic</dd>
</dl>
<hr>
<p>Draw the graphic</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_pred(self, x, y, var1, var2, cmap=None,
              vmin=None, vmax=None, add_points=True,
              add_score=False):
    &#34;&#34;&#34;
    Plot the prediction zone of rules in a 2D plot

    Parameters
    ----------
    x : {array-like, sparse matrix}, shape=[n_samples, n_features]
        Features matrix, where n_samples in the number of samples and
        n_features is the number of features.

    y : {array-like}, shape=[n_samples]
        Target vector relative to X

    var1 : {int type}
           Number of the column of the first variable

    var2 : {int type}
           Number of the column of the second variable

    cmap : {colormap object}, optional
           Colormap used for the graphic

    vmax, vmin : {float type}, optional
                 Parameter of the range of the colorbar

    add_points: {boolean type}, optional
                Option to add the discrete scatter of y

    add_score : {boolean type}, optional
                Option to add the score on the graphic

    -------
    Draw the graphic
    &#34;&#34;&#34;
    nb_bucket = self.get_param(&#39;nb_bucket&#39;)
    x_discretized = self.discretize(x)
    selected_rs = self.get_param(&#39;selected_rs&#39;)
    y_train = self.get_param(&#39;y&#39;)
    ymean = self.get_param(&#39;ymean&#39;)
    ystd = self.get_param(&#39;ystd&#39;)

    x1 = x_discretized[:, var1]
    x2 = x_discretized[:, var2]

    xx, yy = np.meshgrid(range(nb_bucket),
                         range(nb_bucket))

    if cmap is None:
        cmap = plt.cm.get_cmap(&#39;coolwarm&#39;)

    z = selected_rs.predict(y_train, np.c_[np.round(xx.ravel()),
                                           np.round(yy.ravel())],
                            ymean, ystd)

    if vmin is None:
        vmin = min(z)
    if vmax is None:
        vmax = max(z)

    z = z.reshape(xx.shape)

    plt.contourf(xx, yy, z, cmap=cmap, alpha=.8, vmax=vmax, vmin=vmin)

    if add_points:
        area = map(lambda b:
                   map(lambda a:
                       np.extract(np.logical_and(x1 == a, x2 == b),
                                  y).mean(), range(nb_bucket)),
                   range(nb_bucket))
        area = list(area)

        area_len = map(lambda b:
                       map(lambda a:
                           len(np.extract(np.logical_and(x1 == a, x2 == b),
                                          y)) * 10, range(nb_bucket)),
                       range(nb_bucket))
        area_len = list(area_len)

        plt.scatter(xx, yy, c=area, s=area_len, alpha=1.0,
                    cmap=cmap, vmax=vmax, vmin=vmin)

    plt.title(&#39;RIPE prediction&#39;)

    if add_score:
        score = self.score(x, y)
        plt.text(nb_bucket - .70, .08, (&#39;%.2f&#39; % str(score)).lstrip(&#39;0&#39;),
                 size=20, horizontalalignment=&#39;right&#39;)

    plt.axis([-0.01, nb_bucket - 0.99, -0.01, nb_bucket - 0.99])
    plt.colorbar()</code></pre>
</details>
</dd>
<dt id="RICE.Learning.plot_rules"><code class="name flex">
<span>def <span class="ident">plot_rules</span></span>(<span>self, var1, var2, length=None, col_pos='red', col_neg='blue')</span>
</code></dt>
<dd>
<div class="desc"><p>Plot the rectangle activation zone of rules in a 2D plot
the color is corresponding to the intensity of the prediction</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>var1</code></strong> :&ensp;<code>{string type}</code></dt>
<dd>Name of the first variable</dd>
<dt><strong><code>var2</code></strong> :&ensp;<code>{string type}</code></dt>
<dd>Name of the second variable</dd>
<dt><strong><code>length</code></strong> :&ensp;<code>{int type}</code>, optional</dt>
<dd>Option to plot only the length 1 or length 2 rules</dd>
<dt><strong><code>col_pos</code></strong> :&ensp;<code>{string type}</code>, optional<code>,</code></dt>
<dd>Name of the color of the zone of positive rules</dd>
<dt><strong><code>col_neg</code></strong> :&ensp;<code>{string type}</code>, optional</dt>
<dd>Name of the color of the zone of negative rules</dd>
</dl>
<hr>
<p>Draw the graphic</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_rules(self, var1, var2, length=None,
               col_pos=&#39;red&#39;, col_neg=&#39;blue&#39;):
    &#34;&#34;&#34;
    Plot the rectangle activation zone of rules in a 2D plot
    the color is corresponding to the intensity of the prediction

    Parameters
    ----------
    var1 : {string type}
           Name of the first variable

    var2 : {string type}
           Name of the second variable

    length : {int type}, optional
             Option to plot only the length 1 or length 2 rules

    col_pos : {string type}, optional,
              Name of the color of the zone of positive rules

    col_neg : {string type}, optional
              Name of the color of the zone of negative rules

    -------
    Draw the graphic
    &#34;&#34;&#34;
    selected_rs = self.get_param(&#39;selected_rs&#39;)
    nb_bucket = self.get_param(&#39;nb_bucket&#39;)

    if length is not None:
        sub_ruleset = selected_rs.extract_length(length)
    else:
        sub_ruleset = selected_rs

    plt.plot()

    for rule in sub_ruleset:
        rule_condition = rule.conditions

        var = rule_condition.get_param(&#39;features_index&#39;)
        bmin = rule_condition.get_param(&#39;bmin&#39;)
        bmax = rule_condition.get_param(&#39;bmax&#39;)
        length_rule = rule.get_param(&#39;length&#39;)

        if rule.get_param(&#39;pred&#39;) &gt; 0:
            hatch = &#39;/&#39;
            facecolor = col_pos
            alpha = min(1, abs(rule.get_param(&#39;pred&#39;)) / 2.0)
        else:
            hatch = &#39;\\&#39;
            facecolor = col_neg
            alpha = min(1, abs(rule.get_param(&#39;pred&#39;)) / 2.0)

        if length_rule == 1:
            if var[0] == var1:
                p = patches.Rectangle((bmin[0], 0),  # origin
                                      (bmax[0] - bmin[0]) + 0.99,  # width
                                      nb_bucket,  # height
                                      hatch=hatch, facecolor=facecolor,
                                      alpha=alpha)
                plt.gca().add_patch(p)

            elif var[0] == var2:
                p = patches.Rectangle((0, bmin[0]),
                                      nb_bucket,
                                      (bmax[0] - bmin[0]) + 0.99,
                                      hatch=hatch, facecolor=facecolor,
                                      alpha=alpha)
                plt.gca().add_patch(p)

        elif length_rule == 2:
            if var[0] == var1 and var[1] == var2:
                p = patches.Rectangle((bmin[0], bmin[1]),
                                      (bmax[0] - bmin[0]) + 0.99,
                                      (bmax[1] - bmin[1]) + 0.99,
                                      hatch=hatch, facecolor=facecolor,
                                      alpha=alpha)
                plt.gca().add_patch(p)

            elif var[1] == var1 and var[0] == var2:
                p = patches.Rectangle((bmin[1], bmin[0]),
                                      (bmax[1] - bmin[1]) + 0.99,
                                      (bmax[0] - bmin[0]) + 0.99,
                                      hatch=hatch, facecolor=facecolor,
                                      alpha=alpha)
                plt.gca().add_patch(p)

    if length is None:
        plt.gca().set_title(&#39;rules activations&#39;)
    else:
        plt.gca().set_title(&#39;rules l%s activations&#39; % str(length))

    plt.gca().axis([-0.1, nb_bucket + 0.1, -0.1, nb_bucket + 0.1])</code></pre>
</details>
</dd>
<dt id="RICE.Learning.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, X, check_input=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Predict regression target for X.
The predicted regression target of an input sample is computed as the
application of the selected ruleset on X.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>{array type</code> or <code>sparse matrix</code> of <code>shape = [n_samples, n_features]}</code></dt>
<dd>The input samples. Internally, its dtype will be converted to
<code>dtype=np.float32</code>. If a spares matrix is provided, it will be
converted into a spares <code>csr_matrix</code>.</dd>
<dt><strong><code>check_input</code></strong> :&ensp;<code>bool type</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>y</code></strong> :&ensp;<code>{array type</code> of <code>shape = [n_samples]}</code></dt>
<dd>The predicted values.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, X, check_input=True):
    &#34;&#34;&#34;
    Predict regression target for X.
    The predicted regression target of an input sample is computed as the
    application of the selected ruleset on X.

    Parameters
    ----------
    X : {array type or sparse matrix of shape = [n_samples, n_features]}
        The input samples. Internally, its dtype will be converted to
        ``dtype=np.float32``. If a spares matrix is provided, it will be
        converted into a spares ``csr_matrix``.

    check_input : bool type

    Returns
    -------
    y : {array type of shape = [n_samples]}
        The predicted values.
    &#34;&#34;&#34;
    y_train = self.get_param(&#39;y&#39;)
    x_train = self.get_param(&#39;X&#39;)

    X = self.validate_X_predict(X, check_input)
    x_copy = self.discretize(X)

    ruleset = self.get_param(&#39;selected_rs&#39;)

    prediction_vector, bad_cells, no_rules = ruleset.predict(y_train, x_train, x_copy)

    return np.array(prediction_vector), bad_cells, no_rules</code></pre>
</details>
</dd>
<dt id="RICE.Learning.score"><code class="name flex">
<span>def <span class="ident">score</span></span>(<span>self, x, y, sample_weight=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the coefficient of determination R^2 of the prediction
if y is continuous. Else if y in {0,1} then Returns the mean
accuracy on the given test data and labels {0,1}.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>{array type</code> or <code>sparse matrix</code> of <code>shape = [n_samples, n_features]}</code></dt>
<dd>Test samples.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>{array type</code> of <code>shape = [n_samples]}</code></dt>
<dd>True values for y.</dd>
<dt><strong><code>sample_weight</code></strong> :&ensp;<code>{array type</code> of <code>shape = [n_samples]</code>, optional</dt>
<dd>Sample weights.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>score</code></strong> :&ensp;<code>float</code></dt>
<dd>R^2 of self.predict(X) wrt. y in R.
or</dd>
<dt><strong><code>score</code></strong> :&ensp;<code>float</code></dt>
<dd>Mean accuracy of self.predict(X) wrt. y in {0,1}</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def score(self, x, y, sample_weight=None):
    &#34;&#34;&#34;
    Returns the coefficient of determination R^2 of the prediction
    if y is continuous. Else if y in {0,1} then Returns the mean
    accuracy on the given test data and labels {0,1}.

    Parameters
    ----------
    x : {array type or sparse matrix of shape = [n_samples, n_features]}
        Test samples.

    y : {array type of shape = [n_samples]}
        True values for y.

    sample_weight : {array type of shape = [n_samples], optional
        Sample weights.

    Returns
    -------
    score : float
        R^2 of self.predict(X) wrt. y in R.
        or
    score : float
        Mean accuracy of self.predict(X) wrt. y in {0,1}
    &#34;&#34;&#34;
    x_copy = copy.copy(x)

    prediction_vector = self.predict(x_copy)
    prediction_vector = np.nan_to_num(prediction_vector)

    nan_val = np.argwhere(np.isnan(y))
    if len(nan_val) &gt; 0:
        prediction_vector = np.delete(prediction_vector, nan_val)
        y = np.delete(y, nan_val)

    if len(set(y)) == 2:
        th_val = (min(y) + max(y)) / 2.0
        prediction_vector = list(map(lambda p: min(y) if p &lt; th_val else max(y),
                                     prediction_vector))
        return accuracy_score(y, prediction_vector)
    else:
        return r2_score(y, prediction_vector, sample_weight=sample_weight,
                        multioutput=&#39;variance_weighted&#39;)</code></pre>
</details>
</dd>
<dt id="RICE.Learning.select"><code class="name flex">
<span>def <span class="ident">select</span></span>(<span>self, rs, selected_rs=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select(self, rs, selected_rs=None):
    # y_train = self.get_param(&#39;y&#39;)
    # calcmethod = self.get_param(&#39;calcmethod&#39;)
    # crit_evo = self.get_param(&#39;critlist&#39;)
    low_memory = self.get_param(&#39;low_memory&#39;)
    if low_memory:
        x_train = self.get_param(&#39;X&#39;)
    else:
        x_train = None

    if selected_rs is None:
        selected_rs = RuleSet(rs[:1])
        gamma = self.get_param(&#39;gamma&#39;)
        i = 1
        rg_add = 1
    else:
        # gamma = self.get_param(&#39;gamma&#39;)
        gamma = 1.0
        i = 0
        rg_add = 0
    # old_criterion = calc_ruleset_crit(selected_rs, y_train, x_train, calcmethod)
    # crit_evo.append(old_criterion)
    nb_rules = len(rs)

    activation_rs = selected_rs.calc_activation(x_train)
    if low_memory:
        [r.set_params(activation=r.get_activation(x_train)) for r in selected_rs]
    else:
        pass

    while calc_coverage(activation_rs) &lt; 1 and i &lt; nb_rules:
        rs_copy = copy.deepcopy(selected_rs)
        new_rules = rs[i]
        if low_memory:
            new_rules.set_params(activation=new_rules.get_activation(x_train))
        else:
            pass
        union_tests = [new_rules.union_test(rule.get_activation(x_train),
                                            gamma, x_train)
                       for rule in rs_copy]

        if all(union_tests) and \
                new_rules.union_test(activation_rs, gamma, x_train):
            new_rs = copy.deepcopy(selected_rs)
            new_rs.append(new_rules)
            # new_criterion = calc_ruleset_crit(new_rs, y_train, x_train, calcmethod)

            selected_rs = copy.deepcopy(new_rs)
            activation_rs = selected_rs.calc_activation(x_train)
            # old_criterion = new_criterion
            rg_add += 1

        # crit_evo.append(old_criterion)
        i += 1

    # self.set_params(critlist=crit_evo)
    return rg_add, selected_rs</code></pre>
</details>
</dd>
<dt id="RICE.Learning.select_rules"><code class="name flex">
<span>def <span class="ident">select_rules</span></span>(<span>self, length)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a subset of a given ruleset.
This subset minimizes the empirical contrast on the learning set</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_rules(self, length):
    &#34;&#34;&#34;
    Returns a subset of a given ruleset.
    This subset minimizes the empirical contrast on the learning set
    &#34;&#34;&#34;
    ymean = self.get_param(&#39;ymean&#39;)
    # ystd = self.get_param(&#39;ystd&#39;)
    ruleset = self.get_param(&#39;ruleset&#39;)
    beta = self.get_param(&#39;beta&#39;)
    epsilon = self.get_param(&#39;epsilon&#39;)

    x_train = self.get_param(&#39;X&#39;)

    if length &gt; 0:
        sub_ruleset = ruleset.extract_length(length)
    else:
        sub_ruleset = copy.deepcopy(ruleset)

    print(&#39;Number of rules: %s&#39; % str(len(sub_ruleset)))

    if hasattr(self, &#39;sigma&#39;):
        sigma = self.get_param(&#39;sigma&#39;)
    else:
        sigma = min(sub_ruleset.get_rules_param(&#39;var&#39;))
        self.set_params(sigma=sigma)

    significant_list = list(filter(lambda rule: significant_test(rule, ymean,
                                                                 sigma, beta),
                                   sub_ruleset))
    [rule.set_params(significant=True) for rule in significant_list]
    significant_ruleset = RuleSet(significant_list)
    print(&#39;Number of rules after significant test: %s&#39;
          % str(len(significant_ruleset)))

    if len(significant_ruleset) &gt; 0:
        significant_ruleset.sort_by(&#39;cov&#39;, True)
        # significant_ruleset.sort_by(&#39;crit&#39;, False)
        rg_add, selected_rs = self.select(significant_ruleset)
        print(&#39;Number of selected significant rules: %s&#39; % str(rg_add))

    else:
        selected_rs = None
        print(&#39;No significant rules selected!&#39;)

    if self.coverage:
        # Add insignificant rules
        if selected_rs is None or selected_rs.calc_coverage(x_train) &lt; 1:
            insignificant_list = filter(lambda rule: insignificant_test(rule, sigma,
                                                                        epsilon),
                                        sub_ruleset)
            insignificant_list = list(filter(lambda rule:
                                             rule not in significant_list,
                                             insignificant_list))
            if len(list(insignificant_list)) &gt; 0:
                [rule.set_params(significant=False) for rule in insignificant_list]
                insignificant_ruleset = RuleSet(insignificant_list)
                print(&#39;Number rules after insignificant test: %s&#39;
                      % str(len(insignificant_ruleset)))

                insignificant_ruleset.sort_by(&#39;var&#39;, False)
                rg_add, selected_rs = self.select(insignificant_ruleset, selected_rs)
                print(&#39;Number insignificant rules added: %s&#39; % str(rg_add))
            else:
                print(&#39;No insignificant rule added.&#39;)
        else:
            print(&#39;Covering is completed. No insignificant rule added.&#39;)

        # Add rule to have a covering
        if selected_rs.calc_coverage(x_train) &lt; 1:
            print(&#39;Warning: Covering is not completed!&#39;)
            print(selected_rs.calc_coverage(x_train))
            # neg_rule, pos_rule = add_no_rule(selected_rs, x_train, y_train)
            # features_name = self.get_param(&#39;features_name&#39;)
            #
            # if neg_rule is not None:
            #     id_feature = neg_rule.conditions.get_param(&#39;features_index&#39;)
            #     rule_features = list(itemgetter(*id_feature)(features_name))
            #     neg_rule.conditions.set_params(features_name=rule_features)
            #     neg_rule.calc_stats(y=y_train, x=x_train, cov_min=0.0, cov_max=1.0)
            #     print(&#39;Add negative no-rule  %s.&#39; % str(neg_rule))
            #     selected_rs.append(neg_rule)
            #
            # if pos_rule is not None:
            #     id_feature = pos_rule.conditions.get_param(&#39;features_index&#39;)
            #     rule_features = list(itemgetter(*id_feature)(features_name))
            #     pos_rule.conditions.set_params(features_name=rule_features)
            #     pos_rule.calc_stats(y=y_train, x=x_train, cov_min=0.0, cov_max=1.0)
            #     print(&#39;Add positive no-rule  %s.&#39; % str(pos_rule))
            #     selected_rs.append(pos_rule)
        else:
            print(&#39;Covering is completed.&#39;)

    return selected_rs</code></pre>
</details>
</dd>
<dt id="RICE.Learning.set_params"><code class="name flex">
<span>def <span class="ident">set_params</span></span>(<span>self, **parameters)</span>
</code></dt>
<dd>
<div class="desc"><p>To set a new parameter
Example:</p>
<hr>
<p>o.set_params(new_param=val_new_param)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_params(self, **parameters):
    &#34;&#34;&#34;
    To set a new parameter
    Example:
    --------
    o.set_params(new_param=val_new_param)
    &#34;&#34;&#34;
    for parameter, value in parameters.items():
        setattr(self, parameter, value)
    return self</code></pre>
</details>
</dd>
<dt id="RICE.Learning.validate_X_predict"><code class="name flex">
<span>def <span class="ident">validate_X_predict</span></span>(<span>self, X, check_input)</span>
</code></dt>
<dd>
<div class="desc"><p>Validate X whenever one tries to predict, apply, predict_proba</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validate_X_predict(self, X, check_input):
    &#34;&#34;&#34;
    Validate X whenever one tries to predict, apply, predict_proba
    &#34;&#34;&#34;
    if hasattr(self, &#39;fitted&#39;) is False:
        raise AttributeError(&#34;Estimator not fitted, &#34;
                             &#34;call &#39;fit&#39; before exploiting the model.&#34;)

    if check_input:
        X = check_array(X, dtype=None, force_all_finite=False)  # type: np.ndarray

        n_features = X.shape[1]
        input_features = self.get_param(&#39;features_name&#39;)
        if len(input_features) != n_features:
            raise ValueError(&#34;Number of features of the model must &#34;
                             &#34;match the input. Model n_features is %s and &#34;
                             &#34;input n_features is %s &#34;
                             % (input_features, n_features))

    return X</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="RICE.Rule"><code class="flex name class">
<span>class <span class="ident">Rule</span></span>
<span>(</span><span>rule_conditions)</span>
</code></dt>
<dd>
<div class="desc"><p>Class for a rule with a binary rule condition</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Rule(object):
    &#34;&#34;&#34;
    Class for a rule with a binary rule condition
    &#34;&#34;&#34;

    def __init__(self,
                 rule_conditions):

        assert rule_conditions.__class__ == RuleConditions, \
            &#39;Must be a RuleCondition object&#39;

        self.conditions = rule_conditions
        self.length = len(rule_conditions.get_param(&#39;features_index&#39;))

    def __repr__(self):
        return self.__str__()

    def __eq__(self, other):
        return self.conditions == other.conditions

    def __gt__(self, val):
        return self.get_param(&#39;pred&#39;) &gt; val

    def __lt__(self, val):
        return self.get_param(&#39;pred&#39;) &lt; val

    def __ge__(self, val):
        return self.get_param(&#39;pred&#39;) &gt;= val

    def __le__(self, val):
        return self.get_param(&#39;pred&#39;) &lt;= val

    def __str__(self):
        return &#39;rule: &#39; + self.conditions.__str__()

    def __hash__(self):
        return hash(self.conditions)

    def test_included(self, rule, x=None):
        &#34;&#34;&#34;
        Test to know if a rule (self) and an other (rule)
        are included
        &#34;&#34;&#34;
        activation_self = self.get_activation(x)
        activation_other = rule.get_activation(x)

        intersection = np.logical_and(activation_self, activation_other)

        if np.allclose(intersection, activation_self) \
                or np.allclose(intersection, activation_other):
            return None
        else:
            return 1 * intersection

    def test_variables(self, rule):
        &#34;&#34;&#34;
        Test to know if a rule (self) and an other (rule)
        have conditions on the same features.
        &#34;&#34;&#34;
        c1 = self.conditions
        c2 = rule.conditions

        c1_name = c1.get_param(&#39;features_name&#39;)
        c2_name = c2.get_param(&#39;features_name&#39;)
        if len(set(c1_name).intersection(c2_name)) != 0:
            return True
        else:
            return False

    def test_length(self, rule, length):
        &#34;&#34;&#34;
        Test to know if a rule (self) and an other (rule)
        could be intersected to have a new rule of length length.
        &#34;&#34;&#34;
        return self.get_param(&#39;length&#39;) + rule.get_param(&#39;length&#39;) == length

    def intersect_test(self, rule, X):
        &#34;&#34;&#34;
        Test to know if a rule (self) and an other (rule)
        could be intersected.

        Test 1: the sum of complexities of self and rule are egal to l
        Test 2: self and rule have not condition on the same variable
        Test 3: self and rule have not included activation
        &#34;&#34;&#34;
        if self.test_variables(rule) is False:
            return self.test_included(rule=rule, x=X)
        else:
            return None

    def union_test(self, activation, gamma=0.80, X=None):
        &#34;&#34;&#34;
        Test to know if a rule (self) and an activation vector have
        at more gamma percent of points in common
        &#34;&#34;&#34;
        self_vect = self.get_activation(X)
        intersect_vect = np.logical_and(self_vect, activation)

        pts_inter = np.sum(intersect_vect)
        pts_rule = np.sum(activation)
        pts_self = np.sum(self_vect)

        ans = (pts_inter &lt; gamma * pts_self) and (pts_inter &lt; gamma * pts_rule)

        return ans

    def intersect_conditions(self, rule):
        &#34;&#34;&#34;
        Compute an RuleCondition object from the intersection of an rule
        (self) and an other (rulessert)
        &#34;&#34;&#34;
        conditions_1 = self.conditions
        conditions_2 = rule.conditions

        conditions = list(map(lambda c1, c2: c1 + c2, conditions_1.get_attr(),
                              conditions_2.get_attr()))

        return conditions

    def intersect(self, rule, cov_min, cov_max, X, low_memory):
        &#34;&#34;&#34;
        Compute a suitable rule object from the intersection of an rule
        (self) and an other (rulessert).
        Suitable means that self and rule satisfied the intersection test
        &#34;&#34;&#34;
        new_rule = None
        # if self.get_param(&#39;pred&#39;) * rule.get_param(&#39;pred&#39;) &gt; 0:
        activation = self.intersect_test(rule, X)
        if activation is not None:
            cov = calc_coverage(activation)
            if cov_min &lt;= cov &lt;= cov_max:
                conditions_list = self.intersect_conditions(rule)

                new_conditions = RuleConditions(features_name=conditions_list[0],
                                                features_index=conditions_list[1],
                                                bmin=conditions_list[2],
                                                bmax=conditions_list[3],
                                                xmax=conditions_list[5],
                                                xmin=conditions_list[4])
                new_rule = Rule(new_conditions)
                if low_memory is False:
                    new_rule.set_params(activation=activation)

        return new_rule

    def calc_stats(self, x, y, method=&#39;mse&#39;,
                   cov_min=0.01, cov_max=0.5, low_memory=False):
        &#34;&#34;&#34;
        Calculation of all statistics of an rules

        Parameters
        ----------
        x : {array-like or discretized matrix, shape = [n, d]}
            The training input samples after discretization.

        y : {array-like, shape = [n]}
            The normalized target values (real numbers).

        method : {string type}
                 The method mse_function or msecriterion

        cov_min : {float type such as 0 &lt;= covmin &lt;= 1}, default 0.5
                  The minimal coverage of one rule

        cov_max : {float type such as 0 &lt;= covmax &lt;= 1}, default 0.5
                  The maximal coverage of one rule

        low_memory : {bool type}
                     To save activation vectors of rules

        Return
        ------
        None : if the rule does not verified coverage conditions
        &#34;&#34;&#34;
        self.set_params(out=False)
        activation_vector = self.calc_activation(x=x)

        if sum(activation_vector) &gt; 0:
            if low_memory is False:
                self.set_params(activation=activation_vector)

            cov = calc_coverage(activation_vector)
            self.set_params(cov=cov)

            if cov &gt;= cov_max or cov &lt;= cov_min:
                self.set_params(out=True)
            else:
                prediction = calc_prediction(activation_vector, y)
                self.set_params(pred=prediction)

                cond_var = calc_variance(activation_vector, y)
                self.set_params(var=cond_var)

                prediction_vector = activation_vector * prediction
                complementary_prediction = calc_prediction(1 - activation_vector, y)
                np.place(prediction_vector, prediction_vector == 0,
                         complementary_prediction)

                rez = calc_criterion(prediction_vector, y, method)
                self.set_params(crit=rez)

        else:
            self.set_params(out=True)

    def calc_activation(self, x=None):
        &#34;&#34;&#34;
        Compute the activation vector of an rule
        &#34;&#34;&#34;
        return self.conditions.transform(x)

    def predict(self, x=None):
        &#34;&#34;&#34;
        Compute the prediction of an rule
        &#34;&#34;&#34;
        prediction = self.get_param(&#39;pred&#39;)
        if x is not None:
            activation = self.calc_activation(x=x)
        else:
            activation = self.get_activation()

        return prediction * activation

    def score(self, x, y, sample_weight=None, score_type=&#39;Rate&#39;):
        &#34;&#34;&#34;
        Returns the coefficient of determination R^2 of the prediction
        if y is continuous. Else if y in {0,1} then Returns the mean
        accuracy on the given test data and labels {0,1}.

        Parameters
        ----------
        x : array-like, shape = (n_samples, n_features)
            Test samples.

        y : array-like, shape = (n_samples) or (n_samples, n_outputs)
            True values for X.

        sample_weight : array-like, shape = [n_samples], optional
            Sample weights.

        score_type : string-type

        Returns
        -------
        score : float
            R^2 of self.predict(X) wrt. y in R.

            or

        score : float
            Mean accuracy of self.predict(X) wrt. y in {0,1}
        &#34;&#34;&#34;
        prediction_vector = self.predict(x)

        y = np.extract(np.isfinite(y), y)
        prediction_vector = np.extract(np.isfinite(y), prediction_vector)

        if score_type == &#39;Classification&#39;:
            th_val = (min(y) + max(y)) / 2.0
            prediction_vector = list(map(lambda p: min(y) if p &lt; th_val else max(y),
                                         prediction_vector))
            return accuracy_score(y, prediction_vector)

        elif score_type == &#39;Regression&#39;:
            return r2_score(y, prediction_vector, sample_weight=sample_weight,
                            multioutput=&#39;variance_weighted&#39;)

    def make_name(self, num, learning=None):
        &#34;&#34;&#34;
        Add an attribute name to self

        Parameters
        ----------
        num : int
              index of the rule in an ruleset

        learning : Learning object, default None
                   If leaning is not None the name of self will
                   be defined with the name of learning
        &#34;&#34;&#34;
        name = &#39;R &#39; + str(num)
        length = self.get_param(&#39;length&#39;)
        name += &#39;(&#39; + str(length) + &#39;)&#39;
        prediction = self.get_param(&#39;pred&#39;)
        if prediction &gt; 0:
            name += &#39;+&#39;
        elif prediction &lt; 0:
            name += &#39;-&#39;

        if learning is not None:
            dtstart = learning.get_param(&#39;dtstart&#39;)
            dtend = learning.get_param(&#39;dtend&#39;)
            if dtstart is not None:
                name += str(dtstart) + &#39; &#39;
            if dtend is not None:
                name += str(dtend)

        self.set_params(name=name)

    &#34;&#34;&#34;------   Getters   -----&#34;&#34;&#34;

    def get_param(self, param):
        &#34;&#34;&#34;
        To get the parameter param
        &#34;&#34;&#34;
        assert type(param) == str, &#39;Must be a string&#39;
        assert hasattr(self, param), \
            &#39;self.%s must be calculate before&#39; % param
        return getattr(self, param)

    def get_activation(self, x=None):
        &#34;&#34;&#34;
        To get the activation vector of self.
        If it does not exist the function return None
        &#34;&#34;&#34;
        if x is not None:
            return self.conditions.transform(x)
        else:
            if hasattr(self, &#39;activation&#39;):
                return self.get_param(&#39;activation&#39;)
            else:
                print(&#39;No activation vector for %s&#39; % str(self))
            return None

    def get_predictions_vector(self, x=None):
        &#34;&#34;&#34;
        To get the activation vector of self.
        If it does not exist the function return None
        &#34;&#34;&#34;
        if hasattr(self, &#39;pred&#39;):
            prediction = self.get_param(&#39;pred&#39;)
            if hasattr(self, &#39;activation&#39;):
                return prediction * self.get_param(&#39;activation&#39;)
            else:
                return prediction * self.calc_activation(x)
        else:
            return None

    &#34;&#34;&#34;------   Setters   -----&#34;&#34;&#34;

    def set_params(self, **parameters):
        &#34;&#34;&#34;
        To set a new parameter
        Example:
        --------
        o.set_params(new_param=val_new_param)
        &#34;&#34;&#34;
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        return self</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="RICE.Rule.calc_activation"><code class="name flex">
<span>def <span class="ident">calc_activation</span></span>(<span>self, x=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the activation vector of an rule</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calc_activation(self, x=None):
    &#34;&#34;&#34;
    Compute the activation vector of an rule
    &#34;&#34;&#34;
    return self.conditions.transform(x)</code></pre>
</details>
</dd>
<dt id="RICE.Rule.calc_stats"><code class="name flex">
<span>def <span class="ident">calc_stats</span></span>(<span>self, x, y, method='mse', cov_min=0.01, cov_max=0.5, low_memory=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculation of all statistics of an rules</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>{array-like</code> or <code>discretized matrix, shape = [n, d]}</code></dt>
<dd>The training input samples after discretization.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>{array-like, shape = [n]}</code></dt>
<dd>The normalized target values (real numbers).</dd>
<dt><strong><code>method</code></strong> :&ensp;<code>{string type}</code></dt>
<dd>The method mse_function or msecriterion</dd>
<dt><strong><code>cov_min</code></strong> :&ensp;<code>{float type such as 0 &lt;= covmin &lt;= 1}</code>, default <code>0.5</code></dt>
<dd>The minimal coverage of one rule</dd>
<dt><strong><code>cov_max</code></strong> :&ensp;<code>{float type such as 0 &lt;= covmax &lt;= 1}</code>, default <code>0.5</code></dt>
<dd>The maximal coverage of one rule</dd>
<dt><strong><code>low_memory</code></strong> :&ensp;<code>{bool type}</code></dt>
<dd>To save activation vectors of rules</dd>
</dl>
<h2 id="return">Return</h2>
<p>None : if the rule does not verified coverage conditions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calc_stats(self, x, y, method=&#39;mse&#39;,
               cov_min=0.01, cov_max=0.5, low_memory=False):
    &#34;&#34;&#34;
    Calculation of all statistics of an rules

    Parameters
    ----------
    x : {array-like or discretized matrix, shape = [n, d]}
        The training input samples after discretization.

    y : {array-like, shape = [n]}
        The normalized target values (real numbers).

    method : {string type}
             The method mse_function or msecriterion

    cov_min : {float type such as 0 &lt;= covmin &lt;= 1}, default 0.5
              The minimal coverage of one rule

    cov_max : {float type such as 0 &lt;= covmax &lt;= 1}, default 0.5
              The maximal coverage of one rule

    low_memory : {bool type}
                 To save activation vectors of rules

    Return
    ------
    None : if the rule does not verified coverage conditions
    &#34;&#34;&#34;
    self.set_params(out=False)
    activation_vector = self.calc_activation(x=x)

    if sum(activation_vector) &gt; 0:
        if low_memory is False:
            self.set_params(activation=activation_vector)

        cov = calc_coverage(activation_vector)
        self.set_params(cov=cov)

        if cov &gt;= cov_max or cov &lt;= cov_min:
            self.set_params(out=True)
        else:
            prediction = calc_prediction(activation_vector, y)
            self.set_params(pred=prediction)

            cond_var = calc_variance(activation_vector, y)
            self.set_params(var=cond_var)

            prediction_vector = activation_vector * prediction
            complementary_prediction = calc_prediction(1 - activation_vector, y)
            np.place(prediction_vector, prediction_vector == 0,
                     complementary_prediction)

            rez = calc_criterion(prediction_vector, y, method)
            self.set_params(crit=rez)

    else:
        self.set_params(out=True)</code></pre>
</details>
</dd>
<dt id="RICE.Rule.get_activation"><code class="name flex">
<span>def <span class="ident">get_activation</span></span>(<span>self, x=None)</span>
</code></dt>
<dd>
<div class="desc"><p>To get the activation vector of self.
If it does not exist the function return None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_activation(self, x=None):
    &#34;&#34;&#34;
    To get the activation vector of self.
    If it does not exist the function return None
    &#34;&#34;&#34;
    if x is not None:
        return self.conditions.transform(x)
    else:
        if hasattr(self, &#39;activation&#39;):
            return self.get_param(&#39;activation&#39;)
        else:
            print(&#39;No activation vector for %s&#39; % str(self))
        return None</code></pre>
</details>
</dd>
<dt id="RICE.Rule.get_param"><code class="name flex">
<span>def <span class="ident">get_param</span></span>(<span>self, param)</span>
</code></dt>
<dd>
<div class="desc"><p>To get the parameter param</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_param(self, param):
    &#34;&#34;&#34;
    To get the parameter param
    &#34;&#34;&#34;
    assert type(param) == str, &#39;Must be a string&#39;
    assert hasattr(self, param), \
        &#39;self.%s must be calculate before&#39; % param
    return getattr(self, param)</code></pre>
</details>
</dd>
<dt id="RICE.Rule.get_predictions_vector"><code class="name flex">
<span>def <span class="ident">get_predictions_vector</span></span>(<span>self, x=None)</span>
</code></dt>
<dd>
<div class="desc"><p>To get the activation vector of self.
If it does not exist the function return None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_predictions_vector(self, x=None):
    &#34;&#34;&#34;
    To get the activation vector of self.
    If it does not exist the function return None
    &#34;&#34;&#34;
    if hasattr(self, &#39;pred&#39;):
        prediction = self.get_param(&#39;pred&#39;)
        if hasattr(self, &#39;activation&#39;):
            return prediction * self.get_param(&#39;activation&#39;)
        else:
            return prediction * self.calc_activation(x)
    else:
        return None</code></pre>
</details>
</dd>
<dt id="RICE.Rule.intersect"><code class="name flex">
<span>def <span class="ident">intersect</span></span>(<span>self, rule, cov_min, cov_max, X, low_memory)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute a suitable rule object from the intersection of an rule
(self) and an other (rulessert).
Suitable means that self and rule satisfied the intersection test</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def intersect(self, rule, cov_min, cov_max, X, low_memory):
    &#34;&#34;&#34;
    Compute a suitable rule object from the intersection of an rule
    (self) and an other (rulessert).
    Suitable means that self and rule satisfied the intersection test
    &#34;&#34;&#34;
    new_rule = None
    # if self.get_param(&#39;pred&#39;) * rule.get_param(&#39;pred&#39;) &gt; 0:
    activation = self.intersect_test(rule, X)
    if activation is not None:
        cov = calc_coverage(activation)
        if cov_min &lt;= cov &lt;= cov_max:
            conditions_list = self.intersect_conditions(rule)

            new_conditions = RuleConditions(features_name=conditions_list[0],
                                            features_index=conditions_list[1],
                                            bmin=conditions_list[2],
                                            bmax=conditions_list[3],
                                            xmax=conditions_list[5],
                                            xmin=conditions_list[4])
            new_rule = Rule(new_conditions)
            if low_memory is False:
                new_rule.set_params(activation=activation)

    return new_rule</code></pre>
</details>
</dd>
<dt id="RICE.Rule.intersect_conditions"><code class="name flex">
<span>def <span class="ident">intersect_conditions</span></span>(<span>self, rule)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute an RuleCondition object from the intersection of an rule
(self) and an other (rulessert)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def intersect_conditions(self, rule):
    &#34;&#34;&#34;
    Compute an RuleCondition object from the intersection of an rule
    (self) and an other (rulessert)
    &#34;&#34;&#34;
    conditions_1 = self.conditions
    conditions_2 = rule.conditions

    conditions = list(map(lambda c1, c2: c1 + c2, conditions_1.get_attr(),
                          conditions_2.get_attr()))

    return conditions</code></pre>
</details>
</dd>
<dt id="RICE.Rule.intersect_test"><code class="name flex">
<span>def <span class="ident">intersect_test</span></span>(<span>self, rule, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Test to know if a rule (self) and an other (rule)
could be intersected.</p>
<p>Test 1: the sum of complexities of self and rule are egal to l
Test 2: self and rule have not condition on the same variable
Test 3: self and rule have not included activation</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def intersect_test(self, rule, X):
    &#34;&#34;&#34;
    Test to know if a rule (self) and an other (rule)
    could be intersected.

    Test 1: the sum of complexities of self and rule are egal to l
    Test 2: self and rule have not condition on the same variable
    Test 3: self and rule have not included activation
    &#34;&#34;&#34;
    if self.test_variables(rule) is False:
        return self.test_included(rule=rule, x=X)
    else:
        return None</code></pre>
</details>
</dd>
<dt id="RICE.Rule.make_name"><code class="name flex">
<span>def <span class="ident">make_name</span></span>(<span>self, num, learning=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Add an attribute name to self</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>num</code></strong> :&ensp;<code>int</code></dt>
<dd>index of the rule in an ruleset</dd>
<dt><strong><code>learning</code></strong> :&ensp;<code><a title="RICE.Learning" href="#RICE.Learning">Learning</a> object</code>, default <code>None</code></dt>
<dd>If leaning is not None the name of self will
be defined with the name of learning</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_name(self, num, learning=None):
    &#34;&#34;&#34;
    Add an attribute name to self

    Parameters
    ----------
    num : int
          index of the rule in an ruleset

    learning : Learning object, default None
               If leaning is not None the name of self will
               be defined with the name of learning
    &#34;&#34;&#34;
    name = &#39;R &#39; + str(num)
    length = self.get_param(&#39;length&#39;)
    name += &#39;(&#39; + str(length) + &#39;)&#39;
    prediction = self.get_param(&#39;pred&#39;)
    if prediction &gt; 0:
        name += &#39;+&#39;
    elif prediction &lt; 0:
        name += &#39;-&#39;

    if learning is not None:
        dtstart = learning.get_param(&#39;dtstart&#39;)
        dtend = learning.get_param(&#39;dtend&#39;)
        if dtstart is not None:
            name += str(dtstart) + &#39; &#39;
        if dtend is not None:
            name += str(dtend)

    self.set_params(name=name)</code></pre>
</details>
</dd>
<dt id="RICE.Rule.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, x=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the prediction of an rule</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, x=None):
    &#34;&#34;&#34;
    Compute the prediction of an rule
    &#34;&#34;&#34;
    prediction = self.get_param(&#39;pred&#39;)
    if x is not None:
        activation = self.calc_activation(x=x)
    else:
        activation = self.get_activation()

    return prediction * activation</code></pre>
</details>
</dd>
<dt id="RICE.Rule.score"><code class="name flex">
<span>def <span class="ident">score</span></span>(<span>self, x, y, sample_weight=None, score_type='Rate')</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the coefficient of determination R^2 of the prediction
if y is continuous. Else if y in {0,1} then Returns the mean
accuracy on the given test data and labels {0,1}.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>array-like, shape = (n_samples, n_features)</code></dt>
<dd>Test samples.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>array-like, shape = (n_samples)</code> or <code>(n_samples, n_outputs)</code></dt>
<dd>True values for X.</dd>
<dt><strong><code>sample_weight</code></strong> :&ensp;<code>array-like, shape = [n_samples]</code>, optional</dt>
<dd>Sample weights.</dd>
<dt><strong><code>score_type</code></strong> :&ensp;<code>string-type</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>score</code></strong> :&ensp;<code>float</code></dt>
<dd>
<p>R^2 of self.predict(X) wrt. y in R.</p>
<p>or</p>
</dd>
<dt><strong><code>score</code></strong> :&ensp;<code>float</code></dt>
<dd>Mean accuracy of self.predict(X) wrt. y in {0,1}</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def score(self, x, y, sample_weight=None, score_type=&#39;Rate&#39;):
    &#34;&#34;&#34;
    Returns the coefficient of determination R^2 of the prediction
    if y is continuous. Else if y in {0,1} then Returns the mean
    accuracy on the given test data and labels {0,1}.

    Parameters
    ----------
    x : array-like, shape = (n_samples, n_features)
        Test samples.

    y : array-like, shape = (n_samples) or (n_samples, n_outputs)
        True values for X.

    sample_weight : array-like, shape = [n_samples], optional
        Sample weights.

    score_type : string-type

    Returns
    -------
    score : float
        R^2 of self.predict(X) wrt. y in R.

        or

    score : float
        Mean accuracy of self.predict(X) wrt. y in {0,1}
    &#34;&#34;&#34;
    prediction_vector = self.predict(x)

    y = np.extract(np.isfinite(y), y)
    prediction_vector = np.extract(np.isfinite(y), prediction_vector)

    if score_type == &#39;Classification&#39;:
        th_val = (min(y) + max(y)) / 2.0
        prediction_vector = list(map(lambda p: min(y) if p &lt; th_val else max(y),
                                     prediction_vector))
        return accuracy_score(y, prediction_vector)

    elif score_type == &#39;Regression&#39;:
        return r2_score(y, prediction_vector, sample_weight=sample_weight,
                        multioutput=&#39;variance_weighted&#39;)</code></pre>
</details>
</dd>
<dt id="RICE.Rule.set_params"><code class="name flex">
<span>def <span class="ident">set_params</span></span>(<span>self, **parameters)</span>
</code></dt>
<dd>
<div class="desc"><p>To set a new parameter
Example:</p>
<hr>
<p>o.set_params(new_param=val_new_param)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_params(self, **parameters):
    &#34;&#34;&#34;
    To set a new parameter
    Example:
    --------
    o.set_params(new_param=val_new_param)
    &#34;&#34;&#34;
    for parameter, value in parameters.items():
        setattr(self, parameter, value)
    return self</code></pre>
</details>
</dd>
<dt id="RICE.Rule.test_included"><code class="name flex">
<span>def <span class="ident">test_included</span></span>(<span>self, rule, x=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Test to know if a rule (self) and an other (rule)
are included</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_included(self, rule, x=None):
    &#34;&#34;&#34;
    Test to know if a rule (self) and an other (rule)
    are included
    &#34;&#34;&#34;
    activation_self = self.get_activation(x)
    activation_other = rule.get_activation(x)

    intersection = np.logical_and(activation_self, activation_other)

    if np.allclose(intersection, activation_self) \
            or np.allclose(intersection, activation_other):
        return None
    else:
        return 1 * intersection</code></pre>
</details>
</dd>
<dt id="RICE.Rule.test_length"><code class="name flex">
<span>def <span class="ident">test_length</span></span>(<span>self, rule, length)</span>
</code></dt>
<dd>
<div class="desc"><p>Test to know if a rule (self) and an other (rule)
could be intersected to have a new rule of length length.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_length(self, rule, length):
    &#34;&#34;&#34;
    Test to know if a rule (self) and an other (rule)
    could be intersected to have a new rule of length length.
    &#34;&#34;&#34;
    return self.get_param(&#39;length&#39;) + rule.get_param(&#39;length&#39;) == length</code></pre>
</details>
</dd>
<dt id="RICE.Rule.test_variables"><code class="name flex">
<span>def <span class="ident">test_variables</span></span>(<span>self, rule)</span>
</code></dt>
<dd>
<div class="desc"><p>Test to know if a rule (self) and an other (rule)
have conditions on the same features.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_variables(self, rule):
    &#34;&#34;&#34;
    Test to know if a rule (self) and an other (rule)
    have conditions on the same features.
    &#34;&#34;&#34;
    c1 = self.conditions
    c2 = rule.conditions

    c1_name = c1.get_param(&#39;features_name&#39;)
    c2_name = c2.get_param(&#39;features_name&#39;)
    if len(set(c1_name).intersection(c2_name)) != 0:
        return True
    else:
        return False</code></pre>
</details>
</dd>
<dt id="RICE.Rule.union_test"><code class="name flex">
<span>def <span class="ident">union_test</span></span>(<span>self, activation, gamma=0.8, X=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Test to know if a rule (self) and an activation vector have
at more gamma percent of points in common</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def union_test(self, activation, gamma=0.80, X=None):
    &#34;&#34;&#34;
    Test to know if a rule (self) and an activation vector have
    at more gamma percent of points in common
    &#34;&#34;&#34;
    self_vect = self.get_activation(X)
    intersect_vect = np.logical_and(self_vect, activation)

    pts_inter = np.sum(intersect_vect)
    pts_rule = np.sum(activation)
    pts_self = np.sum(self_vect)

    ans = (pts_inter &lt; gamma * pts_self) and (pts_inter &lt; gamma * pts_rule)

    return ans</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="RICE.RuleConditions"><code class="flex name class">
<span>class <span class="ident">RuleConditions</span></span>
<span>(</span><span>features_name, features_index, bmin, bmax, xmin, xmax, values=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Class for binary rule condition</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RuleConditions(object):
    &#34;&#34;&#34;
    Class for binary rule condition
    &#34;&#34;&#34;

    def __init__(self, features_name, features_index,
                 bmin, bmax, xmin, xmax, values=None):

        assert isinstance(features_name, (tuple, list, np.ndarray)), \
            &#39;Type of parameter must be iterable tuple, list or array&#39; % features_name
        self.features_name = features_name
        length = len(features_name)

        assert isinstance(features_index, (tuple, list, np.ndarray)), \
            &#39;Type of parameter must be iterable tuple, list or array&#39; % features_name
        assert len(features_index) == length, \
            &#39;Parameters must have the same length&#39; % features_name
        self.features_index = features_index

        assert isinstance(bmin, (tuple, list, np.ndarray)), \
            &#39;Type of parameter must be iterable tuple, list or array&#39; % features_name
        assert len(bmin) == length, \
            &#39;Parameters must have the same length&#39; % features_name
        assert isinstance(bmax, (tuple, list, np.ndarray)), \
            &#39;Type of parameter must be iterable tuple, list or array&#39; % features_name
        assert len(bmax) == length, \
            &#39;Parameters must have the same length&#39; % features_name
        if type(bmin[0]) != str:
            assert all(map(lambda a, b: a &lt;= b, bmin, bmax)), \
                &#39;Bmin must be smaller or equal than bmax (%s)&#39; \
                % features_name
        self.bmin = bmin
        self.bmax = bmax

        assert isinstance(xmax, (tuple, list, np.ndarray)), \
            &#39;Type of parameter must be iterable tuple, list or array&#39; % features_name
        assert len(xmax) == length, \
            &#39;Parameters must have the same length&#39; % features_name
        assert isinstance(xmin, (tuple, list, np.ndarray)), \
            &#39;Type of parameter must be iterable tuple, list or array&#39; % features_name
        assert len(xmin) == length, \
            &#39;Parameters must have the same length&#39; % features_name
        self.xmin = xmin
        self.xmax = xmax

        if values is None:
            values = []
        else:
            assert isinstance(values, (tuple, list, np.ndarray)), \
                &#39;Type of parameter must be iterable tuple, list or array&#39; % features_name

        self.values = [values]

    def __repr__(self):
        return self.__str__()

    def __str__(self):
        features = self.features_name
        return &#34;Var: %s, Bmin: %s, Bmax: %s&#34; % (features, self.bmin, self.bmax)

    def __eq__(self, other):
        return self.__hash__() == other.__hash__()

    def __hash__(self):
        to_hash = [(self.features_index[i], self.features_name[i],
                    self.bmin[i], self.bmax[i])
                   for i in range(len(self.features_index))]
        to_hash = frozenset(to_hash)
        return hash(to_hash)

    def transform(self, X):
        &#34;&#34;&#34;
        Transform a matrix xmat into an activation vector.
        It means an array of 0 and 1. 0 if the condition is not
        satisfied and 1 otherwise.

        Parameters
        ----------
        X: {array-like matrix, shape=(n_samples, n_features)}
              Input data

        Returns
        -------
        activation_vector: {array-like matrix, shape=(n_samples, 1)}
                     The activation vector
        &#34;&#34;&#34;
        length = len(self.features_name)
        geq_min = True
        leq_min = True
        not_nan = True
        for i in range(length):
            col_index = self.features_index[i]
            x_col = X[:, col_index]

            # Turn x_col to array
            if len(x_col) &gt; 1:
                x_col = np.squeeze(np.asarray(x_col))

            if type(self.bmin[i]) == str:
                x_col = np.array(x_col, dtype=np.str)

                temp = (x_col == self.bmin[i])
                temp |= (x_col == self.bmax[i])
                geq_min &amp;= temp
                leq_min &amp;= True
                not_nan &amp;= True
            else:
                x_col = np.array(x_col, dtype=np.float)

                x_temp = [self.bmin[i] - 1 if x != x else x for x in x_col]
                geq_min &amp;= np.greater_equal(x_temp, self.bmin[i])

                x_temp = [self.bmax[i] + 1 if x != x else x for x in x_col]
                leq_min &amp;= np.less_equal(x_temp, self.bmax[i])

                not_nan &amp;= np.isfinite(x_col)

        activation_vector = 1 * (geq_min &amp; leq_min &amp; not_nan)

        return activation_vector

    &#34;&#34;&#34;------   Getters   -----&#34;&#34;&#34;

    def get_param(self, param):
        &#34;&#34;&#34;
        To get the parameter param
        &#34;&#34;&#34;
        assert type(param) == str, \
            &#39;Must be a string&#39;

        return getattr(self, param)

    def get_attr(self):
        &#34;&#34;&#34;
        To get a list of attributes of self.
        It is useful to quickly create a RuleConditions
        from intersection of two rules
        &#34;&#34;&#34;
        return [self.features_name,
                self.features_index,
                self.bmin, self.bmax,
                self.xmin, self.xmax]

    &#34;&#34;&#34;------   Setters   -----&#34;&#34;&#34;

    def set_params(self, **parameters):
        &#34;&#34;&#34;
        To set a new parameter
        Example:
        --------
        o.set_params(new_param=val_new_param)
        &#34;&#34;&#34;
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        return self</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="RICE.RuleConditions.get_attr"><code class="name flex">
<span>def <span class="ident">get_attr</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>To get a list of attributes of self.
It is useful to quickly create a RuleConditions
from intersection of two rules</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_attr(self):
    &#34;&#34;&#34;
    To get a list of attributes of self.
    It is useful to quickly create a RuleConditions
    from intersection of two rules
    &#34;&#34;&#34;
    return [self.features_name,
            self.features_index,
            self.bmin, self.bmax,
            self.xmin, self.xmax]</code></pre>
</details>
</dd>
<dt id="RICE.RuleConditions.get_param"><code class="name flex">
<span>def <span class="ident">get_param</span></span>(<span>self, param)</span>
</code></dt>
<dd>
<div class="desc"><p>To get the parameter param</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_param(self, param):
    &#34;&#34;&#34;
    To get the parameter param
    &#34;&#34;&#34;
    assert type(param) == str, \
        &#39;Must be a string&#39;

    return getattr(self, param)</code></pre>
</details>
</dd>
<dt id="RICE.RuleConditions.set_params"><code class="name flex">
<span>def <span class="ident">set_params</span></span>(<span>self, **parameters)</span>
</code></dt>
<dd>
<div class="desc"><p>To set a new parameter
Example:</p>
<hr>
<p>o.set_params(new_param=val_new_param)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_params(self, **parameters):
    &#34;&#34;&#34;
    To set a new parameter
    Example:
    --------
    o.set_params(new_param=val_new_param)
    &#34;&#34;&#34;
    for parameter, value in parameters.items():
        setattr(self, parameter, value)
    return self</code></pre>
</details>
</dd>
<dt id="RICE.RuleConditions.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Transform a matrix xmat into an activation vector.
It means an array of 0 and 1. 0 if the condition is not
satisfied and 1 otherwise.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>{array-like matrix, shape=(n_samples, n_features)}</code></dt>
<dd>Input data</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>activation_vector</code></strong> :&ensp;<code>{array-like matrix, shape=(n_samples, 1)}</code></dt>
<dd>The activation vector</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, X):
    &#34;&#34;&#34;
    Transform a matrix xmat into an activation vector.
    It means an array of 0 and 1. 0 if the condition is not
    satisfied and 1 otherwise.

    Parameters
    ----------
    X: {array-like matrix, shape=(n_samples, n_features)}
          Input data

    Returns
    -------
    activation_vector: {array-like matrix, shape=(n_samples, 1)}
                 The activation vector
    &#34;&#34;&#34;
    length = len(self.features_name)
    geq_min = True
    leq_min = True
    not_nan = True
    for i in range(length):
        col_index = self.features_index[i]
        x_col = X[:, col_index]

        # Turn x_col to array
        if len(x_col) &gt; 1:
            x_col = np.squeeze(np.asarray(x_col))

        if type(self.bmin[i]) == str:
            x_col = np.array(x_col, dtype=np.str)

            temp = (x_col == self.bmin[i])
            temp |= (x_col == self.bmax[i])
            geq_min &amp;= temp
            leq_min &amp;= True
            not_nan &amp;= True
        else:
            x_col = np.array(x_col, dtype=np.float)

            x_temp = [self.bmin[i] - 1 if x != x else x for x in x_col]
            geq_min &amp;= np.greater_equal(x_temp, self.bmin[i])

            x_temp = [self.bmax[i] + 1 if x != x else x for x in x_col]
            leq_min &amp;= np.less_equal(x_temp, self.bmax[i])

            not_nan &amp;= np.isfinite(x_col)

    activation_vector = 1 * (geq_min &amp; leq_min &amp; not_nan)

    return activation_vector</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="RICE.RuleSet"><code class="flex name class">
<span>class <span class="ident">RuleSet</span></span>
<span>(</span><span>rs)</span>
</code></dt>
<dd>
<div class="desc"><p>Class for a ruleset. It's a kind of list of rule object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RuleSet(object):
    &#34;&#34;&#34;
    Class for a ruleset. It&#39;s a kind of list of rule object
    &#34;&#34;&#34;

    def __init__(self, rs):
        if type(rs) in [list, np.ndarray]:
            self.rules = rs
        elif type(rs) == RuleSet:
            self.rules = rs.get_rules()

    def __repr__(self):
        return self.__str__()

    def __str__(self):
        return &#39;ruleset: %s rules&#39; % str(len(self.rules))

    def __gt__(self, val):
        return [rule &gt; val for rule in self.rules]

    def __lt__(self, val):
        return [rule &lt; val for rule in self.rules]

    def __ge__(self, val):
        return [rule &gt;= val for rule in self.rules]

    def __le__(self, val):
        return [rule &lt;= val for rule in self.rules]

    def __add__(self, ruleset):
        return self.extend(ruleset)

    def __getitem__(self, i):
        return self.get_rules()[i]

    def __len__(self):
        return len(self.get_rules())

    def __del__(self):
        if len(self) &gt; 0:
            nb_rules = len(self)
            i = 0
            while i &lt; nb_rules:
                del self[0]
                i += 1

    def __delitem__(self, rules_id):
        del self.rules[rules_id]

    def append(self, rule):
        &#34;&#34;&#34;
        Add one rule to a RuleSet object (self).
        &#34;&#34;&#34;
        assert rule.__class__ == Rule, &#39;Must be a rule object (try extend)&#39;
        if any(map(lambda r: rule == r, self)) is False:
            self.rules.append(rule)

    def extend(self, ruleset):
        &#34;&#34;&#34;
        Add rules form a ruleset to a RuleSet object (self).
        &#34;&#34;&#34;
        assert ruleset.__class__ == RuleSet, &#39;Must be a ruleset object&#39;
        &#39;ruleset must have the same Learning object&#39;
        rules_list = ruleset.get_rules()
        self.rules.extend(rules_list)
        return self

    def insert(self, idx, rule):
        &#34;&#34;&#34;
        Insert one rule to a RuleSet object (self) at the position idx.
        &#34;&#34;&#34;
        assert rule.__class__ == Rule, &#39;Must be a rule object&#39;
        self.rules.insert(idx, rule)

    def pop(self, idx=None):
        &#34;&#34;&#34;
        Drop the rule at the position idx.
        &#34;&#34;&#34;
        self.rules.pop(idx)

    def extract_greater(self, param, val):
        &#34;&#34;&#34;
        Extract a RuleSet object from self such as each rules have a param
        greater than val.
        &#34;&#34;&#34;
        rules_list = list(filter(lambda rule: rule.get_param(param) &gt; val, self))
        return RuleSet(rules_list)

    def extract_least(self, param, val):
        &#34;&#34;&#34;
        Extract a RuleSet object from self such as each rules have a param
        least than val.
        &#34;&#34;&#34;
        rules_list = list(filter(lambda rule: rule.get_param(param) &lt; val, self))
        return RuleSet(rules_list)

    def extract_length(self, length):
        &#34;&#34;&#34;
        Extract a RuleSet object from self such as each rules have a
        length l.
        &#34;&#34;&#34;
        rules_list = list(filter(lambda rule: rule.get_param(&#39;length&#39;) == length, self))
        return RuleSet(rules_list)

    def extract(self, param, val):
        &#34;&#34;&#34;
        Extract a RuleSet object from self such as each rules have a param
        equal to val.
        &#34;&#34;&#34;
        rules_list = list(filter(lambda rule: rule.get_param(param) == val, self))
        return RuleSet(rules_list)

    def index(self, rule):
        &#34;&#34;&#34;
        Get the index a rule in a RuleSet object (self).
        &#34;&#34;&#34;
        assert rule.__class__ == Rule, &#39;Must be a rule object&#39;
        self.get_rules().index(rule)

    def replace(self, idx, rule):
        &#34;&#34;&#34;
        Replace rule at position idx in a RuleSet object (self)
        by a new rule.
        &#34;&#34;&#34;
        self.rules.pop(idx)
        self.rules.insert(idx, rule)

    def sort_by(self, crit, maximized):
        &#34;&#34;&#34;
        Sort the RuleSet object (self) by a criteria criterion
        &#34;&#34;&#34;
        self.rules = sorted(self.rules, key=lambda x: x.get_param(crit),
                            reverse=maximized)

    def drop_duplicates(self):
        &#34;&#34;&#34;
        Drop duplicates rules in RuleSet object (self)
        &#34;&#34;&#34;
        rules_list = list(set(self.rules))
        return RuleSet(rules_list)

    def to_df(self, cols=None):
        &#34;&#34;&#34;
        To transform an ruleset into a pandas DataFrame
        &#34;&#34;&#34;
        if cols is None:
            cols = [&#39;Features_Name&#39;, &#39;BMin&#39;, &#39;BMax&#39;,
                    &#39;Cov&#39;, &#39;Pred&#39;, &#39;Var&#39;, &#39;Crit&#39;, &#39;Significant&#39;]

        df = pd.DataFrame(index=self.get_rules_name(),
                          columns=cols)

        for col_name in cols:
            att_name = col_name.lower()
            if all([hasattr(rule, att_name) for rule in self]):
                df[col_name] = [rule.get_param(att_name) for rule in self]

            elif all([hasattr(rule.conditions, att_name.lower()) for rule in self]):
                df[col_name] = [rule.conditions.get_param(att_name) for rule in self]

        return df

    def calc_pred(self, y_train, x_train=None, x_test=None):
        &#34;&#34;&#34;
        Computes the prediction vector
        using an rule based partition
        &#34;&#34;&#34;
        # Activation of all rules in the learning set
        activation_matrix = np.array([rule.get_activation(x_train) for rule in self])

        if x_test is None:
            prediction_matrix = activation_matrix.T
        else:
            prediction_matrix = [rule.calc_activation(x_test) for rule in self]
            prediction_matrix = np.array(prediction_matrix).T

        no_activation_matrix = np.logical_not(prediction_matrix)

        nb_rules_active = prediction_matrix.sum(axis=1)
        nb_rules_active[nb_rules_active == 0] = -1  # If no rule is activated

        # Activation of the intersection of all NOT activated rules at each row
        no_activation_vector = np.dot(no_activation_matrix, activation_matrix)
        no_activation_vector = np.array(no_activation_vector,
                                        dtype=&#39;int&#39;)

        dot_activation = np.dot(prediction_matrix, activation_matrix)
        dot_activation = np.array([np.equal(act, nb_rules) for act, nb_rules in
                                   zip(dot_activation, nb_rules_active)], dtype=&#39;int&#39;)

        # Calculation of the binary vector for cells of the partition et each row
        cells = ((dot_activation - no_activation_vector) &gt; 0)

        # Calculation of the expectation of the complementary
        no_act = 1 - self.calc_activation(x_train)
        no_pred = np.mean(np.extract(y_train, no_act))

        # Get empty significant cells
        significant_list = np.array(self.get_rules_param(&#39;significant&#39;), dtype=int)
        significant_rules = np.where(significant_list == 1)[0]
        temp = prediction_matrix[:, significant_rules]
        nb_rules_active = temp.sum(axis=1)
        nb_rules_active[nb_rules_active == 0] = -1
        empty_cells = np.where(nb_rules_active == -1)[0]

        # Get empty insignificant cells
        bad_cells = np.where(np.sum(cells, axis=1) == 0)[0]
        bad_cells = list(filter(lambda i: i not in empty_cells, bad_cells))

        # Calculation of the conditional expectation in each cell
        prediction_vector = [calc_prediction(act, y_train) for act in cells]
        prediction_vector = np.array(prediction_vector)

        prediction_vector[bad_cells] = no_pred
        prediction_vector[empty_cells] = 0.0

        return prediction_vector, bad_cells, empty_cells

    def calc_activation(self, x=None):
        &#34;&#34;&#34;
        Compute the  activation vector of a set of rules
        &#34;&#34;&#34;
        activation_vector = [rule.get_activation(x) for rule in self]
        activation_vector = np.sum(activation_vector, axis=0)
        activation_vector = 1 * activation_vector.astype(&#39;bool&#39;)

        return activation_vector

    def calc_coverage(self, x=None):
        &#34;&#34;&#34;
        Compute the coverage rate of a set of rules
        &#34;&#34;&#34;
        if len(self) &gt; 0:
            activation_vector = self.calc_activation(x)
            cov = calc_coverage(activation_vector)
        else:
            cov = 0.0
        return cov

    def predict(self, y_train, x_train, x_test):
        &#34;&#34;&#34;
        Computes the prediction vector for a given X and a given aggregation method
        &#34;&#34;&#34;
        prediction_vector, bad_cells, no_rules = self.calc_pred(y_train, x_train, x_test)
        return prediction_vector, bad_cells, no_rules

    def make_rule_names(self):
        &#34;&#34;&#34;
        Add an attribute name at each rule of self
        &#34;&#34;&#34;
        list(map(lambda rule, rules_id: rule.make_name(rules_id),
                 self, range(len(self))))

    def make_selected_df(self):
        df = self.to_df()

        df.rename(columns={&#34;Cov&#34;: &#34;Coverage&#34;, &#34;Pred&#34;: &#34;Prediction&#34;,
                           &#39;Var&#39;: &#39;Variance&#39;, &#39;Crit&#39;: &#39;Criterion&#39;},
                  inplace=True)

        df[&#39;Conditions&#39;] = [make_condition(rule) for rule in self]
        selected_df = df[[&#39;Conditions&#39;, &#39;Coverage&#39;,
                          &#39;Prediction&#39;, &#39;Variance&#39;,
                          &#39;Criterion&#39;]].copy()

        selected_df[&#39;Coverage&#39;] = selected_df.Coverage.round(2)
        selected_df[&#39;Prediction&#39;] = selected_df.Prediction.round(2)
        selected_df[&#39;Variance&#39;] = selected_df.Variance.round(2)
        selected_df[&#39;Criterion&#39;] = selected_df.Criterion.round(2)

        return selected_df

    def plot_counter_variables(self, nb_max=None):
        counter = get_variables_count(self)

        x_labels = list(map(lambda item: item[0], counter))
        values = list(map(lambda item: item[1], counter))

        f = plt.figure()
        ax = plt.subplot()

        if nb_max is not None:
            x_labels = x_labels[:nb_max]
            values = values[:nb_max]

        g = sns.barplot(y=x_labels, x=values, ax=ax, ci=None)
        g.set(xlim=(0, max(values) + 1), ylabel=&#39;Variable&#39;, xlabel=&#39;Count&#39;)

        return f

    def plot_dist(self, x=None, metric=dist):
        rules_names = self.get_rules_name()

        predictions_vector_list = [rule.get_predictions_vector(x) for rule in self]
        predictions_matrix = np.array(predictions_vector_list)

        distance_vector = scipy_dist.pdist(predictions_matrix, metric=metric)
        distance_matrix = scipy_dist.squareform(distance_vector)

        # Set up the matplotlib figure
        f = plt.figure()
        ax = plt.subplot()

        # Generate a mask for the upper triangle
        mask = np.zeros_like(distance_matrix, dtype=np.bool)
        mask[np.triu_indices_from(mask)] = True

        # Generate a custom diverging colormap
        cmap = sns.diverging_palette(220, 10, as_cmap=True)

        vmax = np.max(distance_matrix)
        vmin = np.min(distance_matrix)
        # center = np.mean(distance_matrix)

        # Draw the heatmap with the mask and correct aspect ratio
        sns.heatmap(distance_matrix, cmap=cmap, ax=ax,
                    vmax=vmax, vmin=vmin, center=1.,
                    square=True, xticklabels=rules_names,
                    yticklabels=rules_names, mask=mask)

        plt.yticks(rotation=0)
        plt.xticks(rotation=90)

        return f

    &#34;&#34;&#34;------   Getters   -----&#34;&#34;&#34;
    def get_candidates(self, X, k, length, method, nb_jobs):
        candidates = []
        for l in [1, length - 1]:
            rs_length_l = self.extract_length(l)
            if method == &#39;cluter&#39;:
                if all(map(lambda rule: hasattr(rule, &#39;cluster&#39;),
                           rs_length_l)) is False:
                    clusters = find_cluster(rs_length_l,
                                            X, k, nb_jobs)
                    self.set_rules_cluster(clusters, l)

                rules_list = []
                for i in range(k):
                    sub_rs = rs_length_l.extract(&#39;cluster&#39;, i)
                    if len(sub_rs) &gt; 0:
                        sub_rs.sort_by(&#39;var&#39;, True)
                        rules_list.append(sub_rs[0])

            elif method == &#39;best&#39;:
                rs_length_l.sort_by(&#39;crit&#39;, False)
                rules_list = rs_length_l[:k]

            else:
                print(&#39;Choose a method among [cluster, best] to select candidat&#39;)
                rules_list = rs_length_l.rules

            candidates.append(RuleSet(rules_list))

        return candidates[0], candidates[1]

    def get_rules_param(self, param):
        &#34;&#34;&#34;
        To get the list of a parameter param of the rules in self
        &#34;&#34;&#34;
        return [rule.get_param(param) for rule in self]

    def get_rules_name(self):
        &#34;&#34;&#34;
        To get the list of the name of rules in self
        &#34;&#34;&#34;
        try:
            return self.get_rules_param(&#39;name&#39;)
        except AssertionError:
            self.make_rule_names()
            return self.get_rules_param(&#39;name&#39;)

    def get_rules(self):
        &#34;&#34;&#34;
        To get the list of rule in self
        &#34;&#34;&#34;
        return self.rules

    &#34;&#34;&#34;------   Setters   -----&#34;&#34;&#34;
    def set_rules(self, rules_list):
        &#34;&#34;&#34;
        To set a list of rule in self
        &#34;&#34;&#34;
        assert type(rules_list) == list, &#39;Must be a list object&#39;
        self.rules = rules_list

    def set_rules_cluster(self, params, length):
        rules_list = list(filter(lambda rule: rule.get_param(&#39;length&#39;) == length, self))
        list(map(lambda rule, rules_id: rule.set_params(cluster=params[rules_id]),
                 rules_list, range(len(rules_list))))
        rules_list += list(filter(lambda rule: rule.get_param(&#39;length&#39;) != length, self))

        self.rules = rules_list</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="RICE.RuleSet.append"><code class="name flex">
<span>def <span class="ident">append</span></span>(<span>self, rule)</span>
</code></dt>
<dd>
<div class="desc"><p>Add one rule to a RuleSet object (self).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def append(self, rule):
    &#34;&#34;&#34;
    Add one rule to a RuleSet object (self).
    &#34;&#34;&#34;
    assert rule.__class__ == Rule, &#39;Must be a rule object (try extend)&#39;
    if any(map(lambda r: rule == r, self)) is False:
        self.rules.append(rule)</code></pre>
</details>
</dd>
<dt id="RICE.RuleSet.calc_activation"><code class="name flex">
<span>def <span class="ident">calc_activation</span></span>(<span>self, x=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the
activation vector of a set of rules</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calc_activation(self, x=None):
    &#34;&#34;&#34;
    Compute the  activation vector of a set of rules
    &#34;&#34;&#34;
    activation_vector = [rule.get_activation(x) for rule in self]
    activation_vector = np.sum(activation_vector, axis=0)
    activation_vector = 1 * activation_vector.astype(&#39;bool&#39;)

    return activation_vector</code></pre>
</details>
</dd>
<dt id="RICE.RuleSet.calc_coverage"><code class="name flex">
<span>def <span class="ident">calc_coverage</span></span>(<span>self, x=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the coverage rate of a set of rules</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calc_coverage(self, x=None):
    &#34;&#34;&#34;
    Compute the coverage rate of a set of rules
    &#34;&#34;&#34;
    if len(self) &gt; 0:
        activation_vector = self.calc_activation(x)
        cov = calc_coverage(activation_vector)
    else:
        cov = 0.0
    return cov</code></pre>
</details>
</dd>
<dt id="RICE.RuleSet.calc_pred"><code class="name flex">
<span>def <span class="ident">calc_pred</span></span>(<span>self, y_train, x_train=None, x_test=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the prediction vector
using an rule based partition</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calc_pred(self, y_train, x_train=None, x_test=None):
    &#34;&#34;&#34;
    Computes the prediction vector
    using an rule based partition
    &#34;&#34;&#34;
    # Activation of all rules in the learning set
    activation_matrix = np.array([rule.get_activation(x_train) for rule in self])

    if x_test is None:
        prediction_matrix = activation_matrix.T
    else:
        prediction_matrix = [rule.calc_activation(x_test) for rule in self]
        prediction_matrix = np.array(prediction_matrix).T

    no_activation_matrix = np.logical_not(prediction_matrix)

    nb_rules_active = prediction_matrix.sum(axis=1)
    nb_rules_active[nb_rules_active == 0] = -1  # If no rule is activated

    # Activation of the intersection of all NOT activated rules at each row
    no_activation_vector = np.dot(no_activation_matrix, activation_matrix)
    no_activation_vector = np.array(no_activation_vector,
                                    dtype=&#39;int&#39;)

    dot_activation = np.dot(prediction_matrix, activation_matrix)
    dot_activation = np.array([np.equal(act, nb_rules) for act, nb_rules in
                               zip(dot_activation, nb_rules_active)], dtype=&#39;int&#39;)

    # Calculation of the binary vector for cells of the partition et each row
    cells = ((dot_activation - no_activation_vector) &gt; 0)

    # Calculation of the expectation of the complementary
    no_act = 1 - self.calc_activation(x_train)
    no_pred = np.mean(np.extract(y_train, no_act))

    # Get empty significant cells
    significant_list = np.array(self.get_rules_param(&#39;significant&#39;), dtype=int)
    significant_rules = np.where(significant_list == 1)[0]
    temp = prediction_matrix[:, significant_rules]
    nb_rules_active = temp.sum(axis=1)
    nb_rules_active[nb_rules_active == 0] = -1
    empty_cells = np.where(nb_rules_active == -1)[0]

    # Get empty insignificant cells
    bad_cells = np.where(np.sum(cells, axis=1) == 0)[0]
    bad_cells = list(filter(lambda i: i not in empty_cells, bad_cells))

    # Calculation of the conditional expectation in each cell
    prediction_vector = [calc_prediction(act, y_train) for act in cells]
    prediction_vector = np.array(prediction_vector)

    prediction_vector[bad_cells] = no_pred
    prediction_vector[empty_cells] = 0.0

    return prediction_vector, bad_cells, empty_cells</code></pre>
</details>
</dd>
<dt id="RICE.RuleSet.drop_duplicates"><code class="name flex">
<span>def <span class="ident">drop_duplicates</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Drop duplicates rules in RuleSet object (self)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def drop_duplicates(self):
    &#34;&#34;&#34;
    Drop duplicates rules in RuleSet object (self)
    &#34;&#34;&#34;
    rules_list = list(set(self.rules))
    return RuleSet(rules_list)</code></pre>
</details>
</dd>
<dt id="RICE.RuleSet.extend"><code class="name flex">
<span>def <span class="ident">extend</span></span>(<span>self, ruleset)</span>
</code></dt>
<dd>
<div class="desc"><p>Add rules form a ruleset to a RuleSet object (self).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extend(self, ruleset):
    &#34;&#34;&#34;
    Add rules form a ruleset to a RuleSet object (self).
    &#34;&#34;&#34;
    assert ruleset.__class__ == RuleSet, &#39;Must be a ruleset object&#39;
    &#39;ruleset must have the same Learning object&#39;
    rules_list = ruleset.get_rules()
    self.rules.extend(rules_list)
    return self</code></pre>
</details>
</dd>
<dt id="RICE.RuleSet.extract"><code class="name flex">
<span>def <span class="ident">extract</span></span>(<span>self, param, val)</span>
</code></dt>
<dd>
<div class="desc"><p>Extract a RuleSet object from self such as each rules have a param
equal to val.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract(self, param, val):
    &#34;&#34;&#34;
    Extract a RuleSet object from self such as each rules have a param
    equal to val.
    &#34;&#34;&#34;
    rules_list = list(filter(lambda rule: rule.get_param(param) == val, self))
    return RuleSet(rules_list)</code></pre>
</details>
</dd>
<dt id="RICE.RuleSet.extract_greater"><code class="name flex">
<span>def <span class="ident">extract_greater</span></span>(<span>self, param, val)</span>
</code></dt>
<dd>
<div class="desc"><p>Extract a RuleSet object from self such as each rules have a param
greater than val.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_greater(self, param, val):
    &#34;&#34;&#34;
    Extract a RuleSet object from self such as each rules have a param
    greater than val.
    &#34;&#34;&#34;
    rules_list = list(filter(lambda rule: rule.get_param(param) &gt; val, self))
    return RuleSet(rules_list)</code></pre>
</details>
</dd>
<dt id="RICE.RuleSet.extract_least"><code class="name flex">
<span>def <span class="ident">extract_least</span></span>(<span>self, param, val)</span>
</code></dt>
<dd>
<div class="desc"><p>Extract a RuleSet object from self such as each rules have a param
least than val.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_least(self, param, val):
    &#34;&#34;&#34;
    Extract a RuleSet object from self such as each rules have a param
    least than val.
    &#34;&#34;&#34;
    rules_list = list(filter(lambda rule: rule.get_param(param) &lt; val, self))
    return RuleSet(rules_list)</code></pre>
</details>
</dd>
<dt id="RICE.RuleSet.extract_length"><code class="name flex">
<span>def <span class="ident">extract_length</span></span>(<span>self, length)</span>
</code></dt>
<dd>
<div class="desc"><p>Extract a RuleSet object from self such as each rules have a
length l.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_length(self, length):
    &#34;&#34;&#34;
    Extract a RuleSet object from self such as each rules have a
    length l.
    &#34;&#34;&#34;
    rules_list = list(filter(lambda rule: rule.get_param(&#39;length&#39;) == length, self))
    return RuleSet(rules_list)</code></pre>
</details>
</dd>
<dt id="RICE.RuleSet.get_candidates"><code class="name flex">
<span>def <span class="ident">get_candidates</span></span>(<span>self, X, k, length, method, nb_jobs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_candidates(self, X, k, length, method, nb_jobs):
    candidates = []
    for l in [1, length - 1]:
        rs_length_l = self.extract_length(l)
        if method == &#39;cluter&#39;:
            if all(map(lambda rule: hasattr(rule, &#39;cluster&#39;),
                       rs_length_l)) is False:
                clusters = find_cluster(rs_length_l,
                                        X, k, nb_jobs)
                self.set_rules_cluster(clusters, l)

            rules_list = []
            for i in range(k):
                sub_rs = rs_length_l.extract(&#39;cluster&#39;, i)
                if len(sub_rs) &gt; 0:
                    sub_rs.sort_by(&#39;var&#39;, True)
                    rules_list.append(sub_rs[0])

        elif method == &#39;best&#39;:
            rs_length_l.sort_by(&#39;crit&#39;, False)
            rules_list = rs_length_l[:k]

        else:
            print(&#39;Choose a method among [cluster, best] to select candidat&#39;)
            rules_list = rs_length_l.rules

        candidates.append(RuleSet(rules_list))

    return candidates[0], candidates[1]</code></pre>
</details>
</dd>
<dt id="RICE.RuleSet.get_rules"><code class="name flex">
<span>def <span class="ident">get_rules</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>To get the list of rule in self</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_rules(self):
    &#34;&#34;&#34;
    To get the list of rule in self
    &#34;&#34;&#34;
    return self.rules</code></pre>
</details>
</dd>
<dt id="RICE.RuleSet.get_rules_name"><code class="name flex">
<span>def <span class="ident">get_rules_name</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>To get the list of the name of rules in self</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_rules_name(self):
    &#34;&#34;&#34;
    To get the list of the name of rules in self
    &#34;&#34;&#34;
    try:
        return self.get_rules_param(&#39;name&#39;)
    except AssertionError:
        self.make_rule_names()
        return self.get_rules_param(&#39;name&#39;)</code></pre>
</details>
</dd>
<dt id="RICE.RuleSet.get_rules_param"><code class="name flex">
<span>def <span class="ident">get_rules_param</span></span>(<span>self, param)</span>
</code></dt>
<dd>
<div class="desc"><p>To get the list of a parameter param of the rules in self</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_rules_param(self, param):
    &#34;&#34;&#34;
    To get the list of a parameter param of the rules in self
    &#34;&#34;&#34;
    return [rule.get_param(param) for rule in self]</code></pre>
</details>
</dd>
<dt id="RICE.RuleSet.index"><code class="name flex">
<span>def <span class="ident">index</span></span>(<span>self, rule)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the index a rule in a RuleSet object (self).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def index(self, rule):
    &#34;&#34;&#34;
    Get the index a rule in a RuleSet object (self).
    &#34;&#34;&#34;
    assert rule.__class__ == Rule, &#39;Must be a rule object&#39;
    self.get_rules().index(rule)</code></pre>
</details>
</dd>
<dt id="RICE.RuleSet.insert"><code class="name flex">
<span>def <span class="ident">insert</span></span>(<span>self, idx, rule)</span>
</code></dt>
<dd>
<div class="desc"><p>Insert one rule to a RuleSet object (self) at the position idx.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def insert(self, idx, rule):
    &#34;&#34;&#34;
    Insert one rule to a RuleSet object (self) at the position idx.
    &#34;&#34;&#34;
    assert rule.__class__ == Rule, &#39;Must be a rule object&#39;
    self.rules.insert(idx, rule)</code></pre>
</details>
</dd>
<dt id="RICE.RuleSet.make_rule_names"><code class="name flex">
<span>def <span class="ident">make_rule_names</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Add an attribute name at each rule of self</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_rule_names(self):
    &#34;&#34;&#34;
    Add an attribute name at each rule of self
    &#34;&#34;&#34;
    list(map(lambda rule, rules_id: rule.make_name(rules_id),
             self, range(len(self))))</code></pre>
</details>
</dd>
<dt id="RICE.RuleSet.make_selected_df"><code class="name flex">
<span>def <span class="ident">make_selected_df</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_selected_df(self):
    df = self.to_df()

    df.rename(columns={&#34;Cov&#34;: &#34;Coverage&#34;, &#34;Pred&#34;: &#34;Prediction&#34;,
                       &#39;Var&#39;: &#39;Variance&#39;, &#39;Crit&#39;: &#39;Criterion&#39;},
              inplace=True)

    df[&#39;Conditions&#39;] = [make_condition(rule) for rule in self]
    selected_df = df[[&#39;Conditions&#39;, &#39;Coverage&#39;,
                      &#39;Prediction&#39;, &#39;Variance&#39;,
                      &#39;Criterion&#39;]].copy()

    selected_df[&#39;Coverage&#39;] = selected_df.Coverage.round(2)
    selected_df[&#39;Prediction&#39;] = selected_df.Prediction.round(2)
    selected_df[&#39;Variance&#39;] = selected_df.Variance.round(2)
    selected_df[&#39;Criterion&#39;] = selected_df.Criterion.round(2)

    return selected_df</code></pre>
</details>
</dd>
<dt id="RICE.RuleSet.plot_counter_variables"><code class="name flex">
<span>def <span class="ident">plot_counter_variables</span></span>(<span>self, nb_max=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_counter_variables(self, nb_max=None):
    counter = get_variables_count(self)

    x_labels = list(map(lambda item: item[0], counter))
    values = list(map(lambda item: item[1], counter))

    f = plt.figure()
    ax = plt.subplot()

    if nb_max is not None:
        x_labels = x_labels[:nb_max]
        values = values[:nb_max]

    g = sns.barplot(y=x_labels, x=values, ax=ax, ci=None)
    g.set(xlim=(0, max(values) + 1), ylabel=&#39;Variable&#39;, xlabel=&#39;Count&#39;)

    return f</code></pre>
</details>
</dd>
<dt id="RICE.RuleSet.plot_dist"><code class="name flex">
<span>def <span class="ident">plot_dist</span></span>(<span>self, x=None, metric=&lt;function dist&gt;)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_dist(self, x=None, metric=dist):
    rules_names = self.get_rules_name()

    predictions_vector_list = [rule.get_predictions_vector(x) for rule in self]
    predictions_matrix = np.array(predictions_vector_list)

    distance_vector = scipy_dist.pdist(predictions_matrix, metric=metric)
    distance_matrix = scipy_dist.squareform(distance_vector)

    # Set up the matplotlib figure
    f = plt.figure()
    ax = plt.subplot()

    # Generate a mask for the upper triangle
    mask = np.zeros_like(distance_matrix, dtype=np.bool)
    mask[np.triu_indices_from(mask)] = True

    # Generate a custom diverging colormap
    cmap = sns.diverging_palette(220, 10, as_cmap=True)

    vmax = np.max(distance_matrix)
    vmin = np.min(distance_matrix)
    # center = np.mean(distance_matrix)

    # Draw the heatmap with the mask and correct aspect ratio
    sns.heatmap(distance_matrix, cmap=cmap, ax=ax,
                vmax=vmax, vmin=vmin, center=1.,
                square=True, xticklabels=rules_names,
                yticklabels=rules_names, mask=mask)

    plt.yticks(rotation=0)
    plt.xticks(rotation=90)

    return f</code></pre>
</details>
</dd>
<dt id="RICE.RuleSet.pop"><code class="name flex">
<span>def <span class="ident">pop</span></span>(<span>self, idx=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Drop the rule at the position idx.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pop(self, idx=None):
    &#34;&#34;&#34;
    Drop the rule at the position idx.
    &#34;&#34;&#34;
    self.rules.pop(idx)</code></pre>
</details>
</dd>
<dt id="RICE.RuleSet.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, y_train, x_train, x_test)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the prediction vector for a given X and a given aggregation method</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, y_train, x_train, x_test):
    &#34;&#34;&#34;
    Computes the prediction vector for a given X and a given aggregation method
    &#34;&#34;&#34;
    prediction_vector, bad_cells, no_rules = self.calc_pred(y_train, x_train, x_test)
    return prediction_vector, bad_cells, no_rules</code></pre>
</details>
</dd>
<dt id="RICE.RuleSet.replace"><code class="name flex">
<span>def <span class="ident">replace</span></span>(<span>self, idx, rule)</span>
</code></dt>
<dd>
<div class="desc"><p>Replace rule at position idx in a RuleSet object (self)
by a new rule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def replace(self, idx, rule):
    &#34;&#34;&#34;
    Replace rule at position idx in a RuleSet object (self)
    by a new rule.
    &#34;&#34;&#34;
    self.rules.pop(idx)
    self.rules.insert(idx, rule)</code></pre>
</details>
</dd>
<dt id="RICE.RuleSet.set_rules"><code class="name flex">
<span>def <span class="ident">set_rules</span></span>(<span>self, rules_list)</span>
</code></dt>
<dd>
<div class="desc"><p>To set a list of rule in self</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_rules(self, rules_list):
    &#34;&#34;&#34;
    To set a list of rule in self
    &#34;&#34;&#34;
    assert type(rules_list) == list, &#39;Must be a list object&#39;
    self.rules = rules_list</code></pre>
</details>
</dd>
<dt id="RICE.RuleSet.set_rules_cluster"><code class="name flex">
<span>def <span class="ident">set_rules_cluster</span></span>(<span>self, params, length)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_rules_cluster(self, params, length):
    rules_list = list(filter(lambda rule: rule.get_param(&#39;length&#39;) == length, self))
    list(map(lambda rule, rules_id: rule.set_params(cluster=params[rules_id]),
             rules_list, range(len(rules_list))))
    rules_list += list(filter(lambda rule: rule.get_param(&#39;length&#39;) != length, self))

    self.rules = rules_list</code></pre>
</details>
</dd>
<dt id="RICE.RuleSet.sort_by"><code class="name flex">
<span>def <span class="ident">sort_by</span></span>(<span>self, crit, maximized)</span>
</code></dt>
<dd>
<div class="desc"><p>Sort the RuleSet object (self) by a criteria criterion</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sort_by(self, crit, maximized):
    &#34;&#34;&#34;
    Sort the RuleSet object (self) by a criteria criterion
    &#34;&#34;&#34;
    self.rules = sorted(self.rules, key=lambda x: x.get_param(crit),
                        reverse=maximized)</code></pre>
</details>
</dd>
<dt id="RICE.RuleSet.to_df"><code class="name flex">
<span>def <span class="ident">to_df</span></span>(<span>self, cols=None)</span>
</code></dt>
<dd>
<div class="desc"><p>To transform an ruleset into a pandas DataFrame</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_df(self, cols=None):
    &#34;&#34;&#34;
    To transform an ruleset into a pandas DataFrame
    &#34;&#34;&#34;
    if cols is None:
        cols = [&#39;Features_Name&#39;, &#39;BMin&#39;, &#39;BMax&#39;,
                &#39;Cov&#39;, &#39;Pred&#39;, &#39;Var&#39;, &#39;Crit&#39;, &#39;Significant&#39;]

    df = pd.DataFrame(index=self.get_rules_name(),
                      columns=cols)

    for col_name in cols:
        att_name = col_name.lower()
        if all([hasattr(rule, att_name) for rule in self]):
            df[col_name] = [rule.get_param(att_name) for rule in self]

        elif all([hasattr(rule.conditions, att_name.lower()) for rule in self]):
            df[col_name] = [rule.conditions.get_param(att_name) for rule in self]

    return df</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="RICE.Covering_tools" href="Covering_tools.html">RICE.Covering_tools</a></code></li>
<li><code><a title="RICE.RICE" href="RICE.html">RICE.RICE</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="RICE.Learning" href="#RICE.Learning">Learning</a></code></h4>
<ul class="">
<li><code><a title="RICE.Learning.calc_length_1" href="#RICE.Learning.calc_length_1">calc_length_1</a></code></li>
<li><code><a title="RICE.Learning.calc_length_c" href="#RICE.Learning.calc_length_c">calc_length_c</a></code></li>
<li><code><a title="RICE.Learning.discretize" href="#RICE.Learning.discretize">discretize</a></code></li>
<li><code><a title="RICE.Learning.find_candidates" href="#RICE.Learning.find_candidates">find_candidates</a></code></li>
<li><code><a title="RICE.Learning.find_rules" href="#RICE.Learning.find_rules">find_rules</a></code></li>
<li><code><a title="RICE.Learning.fit" href="#RICE.Learning.fit">fit</a></code></li>
<li><code><a title="RICE.Learning.get_param" href="#RICE.Learning.get_param">get_param</a></code></li>
<li><code><a title="RICE.Learning.make_count_matrix" href="#RICE.Learning.make_count_matrix">make_count_matrix</a></code></li>
<li><code><a title="RICE.Learning.make_selected_df" href="#RICE.Learning.make_selected_df">make_selected_df</a></code></li>
<li><code><a title="RICE.Learning.plot_counter" href="#RICE.Learning.plot_counter">plot_counter</a></code></li>
<li><code><a title="RICE.Learning.plot_counter_variables" href="#RICE.Learning.plot_counter_variables">plot_counter_variables</a></code></li>
<li><code><a title="RICE.Learning.plot_dist" href="#RICE.Learning.plot_dist">plot_dist</a></code></li>
<li><code><a title="RICE.Learning.plot_intensity" href="#RICE.Learning.plot_intensity">plot_intensity</a></code></li>
<li><code><a title="RICE.Learning.plot_pred" href="#RICE.Learning.plot_pred">plot_pred</a></code></li>
<li><code><a title="RICE.Learning.plot_rules" href="#RICE.Learning.plot_rules">plot_rules</a></code></li>
<li><code><a title="RICE.Learning.predict" href="#RICE.Learning.predict">predict</a></code></li>
<li><code><a title="RICE.Learning.score" href="#RICE.Learning.score">score</a></code></li>
<li><code><a title="RICE.Learning.select" href="#RICE.Learning.select">select</a></code></li>
<li><code><a title="RICE.Learning.select_rules" href="#RICE.Learning.select_rules">select_rules</a></code></li>
<li><code><a title="RICE.Learning.set_params" href="#RICE.Learning.set_params">set_params</a></code></li>
<li><code><a title="RICE.Learning.validate_X_predict" href="#RICE.Learning.validate_X_predict">validate_X_predict</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="RICE.Rule" href="#RICE.Rule">Rule</a></code></h4>
<ul class="">
<li><code><a title="RICE.Rule.calc_activation" href="#RICE.Rule.calc_activation">calc_activation</a></code></li>
<li><code><a title="RICE.Rule.calc_stats" href="#RICE.Rule.calc_stats">calc_stats</a></code></li>
<li><code><a title="RICE.Rule.get_activation" href="#RICE.Rule.get_activation">get_activation</a></code></li>
<li><code><a title="RICE.Rule.get_param" href="#RICE.Rule.get_param">get_param</a></code></li>
<li><code><a title="RICE.Rule.get_predictions_vector" href="#RICE.Rule.get_predictions_vector">get_predictions_vector</a></code></li>
<li><code><a title="RICE.Rule.intersect" href="#RICE.Rule.intersect">intersect</a></code></li>
<li><code><a title="RICE.Rule.intersect_conditions" href="#RICE.Rule.intersect_conditions">intersect_conditions</a></code></li>
<li><code><a title="RICE.Rule.intersect_test" href="#RICE.Rule.intersect_test">intersect_test</a></code></li>
<li><code><a title="RICE.Rule.make_name" href="#RICE.Rule.make_name">make_name</a></code></li>
<li><code><a title="RICE.Rule.predict" href="#RICE.Rule.predict">predict</a></code></li>
<li><code><a title="RICE.Rule.score" href="#RICE.Rule.score">score</a></code></li>
<li><code><a title="RICE.Rule.set_params" href="#RICE.Rule.set_params">set_params</a></code></li>
<li><code><a title="RICE.Rule.test_included" href="#RICE.Rule.test_included">test_included</a></code></li>
<li><code><a title="RICE.Rule.test_length" href="#RICE.Rule.test_length">test_length</a></code></li>
<li><code><a title="RICE.Rule.test_variables" href="#RICE.Rule.test_variables">test_variables</a></code></li>
<li><code><a title="RICE.Rule.union_test" href="#RICE.Rule.union_test">union_test</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="RICE.RuleConditions" href="#RICE.RuleConditions">RuleConditions</a></code></h4>
<ul class="">
<li><code><a title="RICE.RuleConditions.get_attr" href="#RICE.RuleConditions.get_attr">get_attr</a></code></li>
<li><code><a title="RICE.RuleConditions.get_param" href="#RICE.RuleConditions.get_param">get_param</a></code></li>
<li><code><a title="RICE.RuleConditions.set_params" href="#RICE.RuleConditions.set_params">set_params</a></code></li>
<li><code><a title="RICE.RuleConditions.transform" href="#RICE.RuleConditions.transform">transform</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="RICE.RuleSet" href="#RICE.RuleSet">RuleSet</a></code></h4>
<ul class="">
<li><code><a title="RICE.RuleSet.append" href="#RICE.RuleSet.append">append</a></code></li>
<li><code><a title="RICE.RuleSet.calc_activation" href="#RICE.RuleSet.calc_activation">calc_activation</a></code></li>
<li><code><a title="RICE.RuleSet.calc_coverage" href="#RICE.RuleSet.calc_coverage">calc_coverage</a></code></li>
<li><code><a title="RICE.RuleSet.calc_pred" href="#RICE.RuleSet.calc_pred">calc_pred</a></code></li>
<li><code><a title="RICE.RuleSet.drop_duplicates" href="#RICE.RuleSet.drop_duplicates">drop_duplicates</a></code></li>
<li><code><a title="RICE.RuleSet.extend" href="#RICE.RuleSet.extend">extend</a></code></li>
<li><code><a title="RICE.RuleSet.extract" href="#RICE.RuleSet.extract">extract</a></code></li>
<li><code><a title="RICE.RuleSet.extract_greater" href="#RICE.RuleSet.extract_greater">extract_greater</a></code></li>
<li><code><a title="RICE.RuleSet.extract_least" href="#RICE.RuleSet.extract_least">extract_least</a></code></li>
<li><code><a title="RICE.RuleSet.extract_length" href="#RICE.RuleSet.extract_length">extract_length</a></code></li>
<li><code><a title="RICE.RuleSet.get_candidates" href="#RICE.RuleSet.get_candidates">get_candidates</a></code></li>
<li><code><a title="RICE.RuleSet.get_rules" href="#RICE.RuleSet.get_rules">get_rules</a></code></li>
<li><code><a title="RICE.RuleSet.get_rules_name" href="#RICE.RuleSet.get_rules_name">get_rules_name</a></code></li>
<li><code><a title="RICE.RuleSet.get_rules_param" href="#RICE.RuleSet.get_rules_param">get_rules_param</a></code></li>
<li><code><a title="RICE.RuleSet.index" href="#RICE.RuleSet.index">index</a></code></li>
<li><code><a title="RICE.RuleSet.insert" href="#RICE.RuleSet.insert">insert</a></code></li>
<li><code><a title="RICE.RuleSet.make_rule_names" href="#RICE.RuleSet.make_rule_names">make_rule_names</a></code></li>
<li><code><a title="RICE.RuleSet.make_selected_df" href="#RICE.RuleSet.make_selected_df">make_selected_df</a></code></li>
<li><code><a title="RICE.RuleSet.plot_counter_variables" href="#RICE.RuleSet.plot_counter_variables">plot_counter_variables</a></code></li>
<li><code><a title="RICE.RuleSet.plot_dist" href="#RICE.RuleSet.plot_dist">plot_dist</a></code></li>
<li><code><a title="RICE.RuleSet.pop" href="#RICE.RuleSet.pop">pop</a></code></li>
<li><code><a title="RICE.RuleSet.predict" href="#RICE.RuleSet.predict">predict</a></code></li>
<li><code><a title="RICE.RuleSet.replace" href="#RICE.RuleSet.replace">replace</a></code></li>
<li><code><a title="RICE.RuleSet.set_rules" href="#RICE.RuleSet.set_rules">set_rules</a></code></li>
<li><code><a title="RICE.RuleSet.set_rules_cluster" href="#RICE.RuleSet.set_rules_cluster">set_rules_cluster</a></code></li>
<li><code><a title="RICE.RuleSet.sort_by" href="#RICE.RuleSet.sort_by">sort_by</a></code></li>
<li><code><a title="RICE.RuleSet.to_df" href="#RICE.RuleSet.to_df">to_df</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>